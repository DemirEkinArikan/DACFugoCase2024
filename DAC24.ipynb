{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM0P-ndd3isM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets (replace file paths with actual ones)\n",
        "users_train = pd.read_csv(\"/content/users_train.csv\")\n",
        "user_features_train = pd.read_csv(\"/content/user_features_train.csv\")\n",
        "targets_train = pd.read_csv(\"/content/targets_train.csv\")\n",
        "\n",
        "users_test = pd.read_csv(\"/content/users_test.csv\")\n",
        "user_features_test = pd.read_csv(\"/content/user_features_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS79Fa6pUpHS",
        "outputId": "9a8b1cab-a6e2-4e9a-afc9-be31cc9330f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.46.0 slicer-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    # Additional interactions\n",
        "    df.loc[:, 'ad_iap_interaction'] = df['ad_revenue_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'level_ad_interaction'] = df['level_advanced_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'level_iap_interaction'] = df['level_advanced_total'] * df['iap_revenue_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction',\n",
        "    'ad_iap_interaction', 'level_ad_interaction', 'level_iap_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(400, return_sequences=False))(seq_inputs)  # Increased from 300 to 400\n",
        "    x = Dropout(0.2)(x)  # Reduced dropout to 0.2\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(256, activation='relu')(combined)  # Increased from 128 to 256\n",
        "    x = Dropout(0.2)(x)  # Reduced dropout to 0.2\n",
        "    x = Dense(128, activation='relu')(x)  # Increased from 64 to 128\n",
        "    x = Dense(64, activation='relu')(x)  # Increased from 32 to 64\n",
        "    x = Dense(16, activation='relu')(x)  # New Dense layer with 16 units\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00005), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4)  # Reduced factor for more gradual reduction\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=100,\n",
        "    batch_size=528,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNznVxi6bzs9",
        "outputId": "6e249100-cb5e-4930-d739-3a5b247d7a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - loss: 3.0907 - val_loss: 2.4565 - learning_rate: 5.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.3105 - val_loss: 2.2609 - learning_rate: 5.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.3306 - val_loss: 2.2313 - learning_rate: 5.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.1762 - val_loss: 2.1715 - learning_rate: 5.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.7050 - val_loss: 2.2796 - learning_rate: 5.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.1057 - val_loss: 2.1905 - learning_rate: 5.0000e-05\n",
            "Epoch 7/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8885 - val_loss: 2.2635 - learning_rate: 5.0000e-05\n",
            "Epoch 8/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8742 - val_loss: 2.2310 - learning_rate: 5.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.9645 - val_loss: 2.2090 - learning_rate: 1.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.6562 - val_loss: 2.2551 - learning_rate: 1.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.5869 - val_loss: 2.2611 - learning_rate: 1.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.5584 - val_loss: 2.2509 - learning_rate: 1.0000e-05\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
            "Validation RMSE: 1.4736013412475586\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    # Additional interactions\n",
        "    df.loc[:, 'ad_iap_interaction'] = df['ad_revenue_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'level_ad_interaction'] = df['level_advanced_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'level_iap_interaction'] = df['level_advanced_total'] * df['iap_revenue_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction',\n",
        "    'ad_iap_interaction', 'level_ad_interaction', 'level_iap_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(400, return_sequences=False))(seq_inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(256, activation='relu')(combined)\n",
        "    x = BatchNormalization()(x)  # Add batch normalization\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)  # Add batch normalization\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00003), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=9, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4)  # Reduced factor for more gradual reduction\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=100,\n",
        "    batch_size=512,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "JIf8XCVMSvxW",
        "outputId": "f0b9eb9e-c5c3-4015-af17-f2e57ea2f3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 3.9599 - val_loss: 3.7356 - learning_rate: 3.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.7612 - val_loss: 7.8160 - learning_rate: 3.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.6930 - val_loss: 6.2852 - learning_rate: 3.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.7194 - val_loss: 10.6211 - learning_rate: 3.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.4266 - val_loss: 8.3937 - learning_rate: 3.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.6445 - val_loss: 12.2898 - learning_rate: 6.0000e-06\n",
            "Epoch 7/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.4364 - val_loss: 7.9890 - learning_rate: 6.0000e-06\n",
            "Epoch 8/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.7587 - val_loss: 10.3498 - learning_rate: 6.0000e-06\n",
            "Epoch 9/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.9637 - val_loss: 11.4633 - learning_rate: 6.0000e-06\n",
            "Epoch 10/100\n",
            "\u001b[1m1373/1373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 2.1992 - val_loss: 13.0978 - learning_rate: 1.2000e-06\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step\n",
            "Validation RMSE: 1.9327689409255981\n",
            "\u001b[1m11787/18305\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 3ms/step"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-af506c1d05a0>\u001b[0m in \u001b[0;36m<cell line: 145>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Predict on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_eng\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# Create the submission file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_to_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYA_B9XdRp1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h797QQ53Rpx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    # Additional interactions\n",
        "    df.loc[:, 'ad_iap_interaction'] = df['ad_revenue_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'level_ad_interaction'] = df['level_advanced_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'level_iap_interaction'] = df['level_advanced_total'] * df['iap_revenue_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction',\n",
        "    'ad_iap_interaction', 'level_ad_interaction', 'level_iap_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(400, return_sequences=False))(seq_inputs)  # Increased from 300 to 400\n",
        "    x = Dropout(0.2)(x)  # Reduced dropout to 0.2\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(256, activation='relu')(combined)  # Increased from 128 to 256\n",
        "    x = Dropout(0.18)(x)  # Reduced dropout to 0.2\n",
        "    x = Dense(128, activation='relu')(x)  # Increased from 64 to 128\n",
        "    x = Dense(64, activation='relu')(x)  # Increased from 32 to 64\n",
        "    x = Dense(16, activation='relu')(x)  # New Dense layer with 16 units\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00005), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4)  # Reduced factor for more gradual reduction\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=100,\n",
        "    batch_size=528,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "JhkpaCJazqek",
        "outputId": "d602a61c-55c6-413c-df2e-5b2d6e634c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - loss: 3.8481 - val_loss: 2.4237 - learning_rate: 5.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m 169/1332\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 1.4036"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-8d39105d19ea>\u001b[0m in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_eng\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_eng\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    # Additional interactions\n",
        "    df.loc[:, 'ad_iap_interaction'] = df['ad_revenue_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'level_ad_interaction'] = df['level_advanced_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'level_iap_interaction'] = df['level_advanced_total'] * df['iap_revenue_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction',\n",
        "    'ad_iap_interaction', 'level_ad_interaction', 'level_iap_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(400, return_sequences=False))(seq_inputs)  # Increased from 300 to 400\n",
        "    x = Dropout(0.2)(x)  # Reduced dropout to 0.2\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(256, activation='relu')(combined)  # Increased from 128 to 256\n",
        "    x = Dropout(0.2)(x)  # Reduced dropout to 0.2\n",
        "    x = Dense(128, activation='relu')(x)  # Increased from 64 to 128\n",
        "    x = Dense(64, activation='relu')(x)  # Increased from 32 to 64\n",
        "    x = Dense(32, activation='relu')(x)  # Increased from 32 to 64\n",
        "    x = Dense(16, activation='relu')(x)  # New Dense layer with 16 units\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00005), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=11, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4)  # Reduced factor for more gradual reduction\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=100,\n",
        "    batch_size=528,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1qmHQDr8OhSh",
        "outputId": "171af5d7-347b-498e-e69e-095ccc0fdd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - loss: 3.4883 - val_loss: 2.4419 - learning_rate: 5.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 2.2530 - val_loss: 2.2507 - learning_rate: 5.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 2.1054 - val_loss: 2.2452 - learning_rate: 5.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.8878 - val_loss: 2.2925 - learning_rate: 5.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 2.3417 - val_loss: 2.2059 - learning_rate: 5.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.9834 - val_loss: 2.1903 - learning_rate: 5.0000e-05\n",
            "Epoch 7/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.8553 - val_loss: 2.2608 - learning_rate: 5.0000e-05\n",
            "Epoch 8/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.8158 - val_loss: 2.4145 - learning_rate: 5.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 2.1154 - val_loss: 2.3942 - learning_rate: 5.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.8653 - val_loss: 2.3595 - learning_rate: 5.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.8062 - val_loss: 2.4121 - learning_rate: 1.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.7732 - val_loss: 2.3924 - learning_rate: 1.0000e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.6090 - val_loss: 2.4268 - learning_rate: 1.0000e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.6387 - val_loss: 2.4007 - learning_rate: 1.0000e-05\n",
            "Epoch 15/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.8008 - val_loss: 2.4131 - learning_rate: 2.0000e-06\n",
            "Epoch 16/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.4693 - val_loss: 2.4157 - learning_rate: 2.0000e-06\n",
            "Epoch 17/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 1.5090 - val_loss: 2.4214 - learning_rate: 2.0000e-06\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step\n",
            "Validation RMSE: 1.4799919128417969\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-71766d699071>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Engineered features for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mX_test_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_eng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengineered_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# Predict on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  12279\u001b[0m                ['monkey', nan, None]], dtype=object)\n\u001b[1;32m  12280\u001b[0m         \"\"\"\n\u001b[0;32m> 12281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  12282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mas_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m             \u001b[0;31m# The underlying data was copied within _interleave, so no need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0;31m# to further copy if copy=True or setting na_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m                 \u001b[0mitemmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDtypeObj\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_dtype_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dtype_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4688169956207275\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    # Additional interactions\n",
        "    df.loc[:, 'ad_iap_interaction'] = df['ad_revenue_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'level_ad_interaction'] = df['level_advanced_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'level_iap_interaction'] = df['level_advanced_total'] * df['iap_revenue_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction',\n",
        "    'ad_iap_interaction', 'level_ad_interaction', 'level_iap_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    # Increase LSTM units back to 400 and remove L2 regularization from the LSTM\n",
        "    x = Bidirectional(LSTM(400, return_sequences=False))(seq_inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    # Keep L2 regularization in Dense layers only\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined)  # Increased back to 256 units\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)  # Increased to 128 units\n",
        "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    # Compile the model with Adam optimizer\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00002), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=9, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4)  # Reduced factor for more gradual reduction\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=100,\n",
        "    batch_size=528,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU3lN1v0zqbB",
        "outputId": "03e7f9ce-6f71-4017-8cf4-576b945481f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - loss: 7.5453 - val_loss: 4.1500 - learning_rate: 2.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 3.3120 - val_loss: 3.4330 - learning_rate: 2.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 3.1997 - val_loss: 3.1686 - learning_rate: 2.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.7527 - val_loss: 3.0114 - learning_rate: 2.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 3.1133 - val_loss: 2.9395 - learning_rate: 2.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.5316 - val_loss: 2.8155 - learning_rate: 2.0000e-05\n",
            "Epoch 7/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.7270 - val_loss: 2.8270 - learning_rate: 2.0000e-05\n",
            "Epoch 8/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.7172 - val_loss: 2.8860 - learning_rate: 2.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.5695 - val_loss: 2.7032 - learning_rate: 2.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.3521 - val_loss: 2.7963 - learning_rate: 2.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2664 - val_loss: 2.7431 - learning_rate: 2.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2116 - val_loss: 2.6851 - learning_rate: 2.0000e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.9741 - val_loss: 2.6411 - learning_rate: 2.0000e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2757 - val_loss: 2.7126 - learning_rate: 2.0000e-05\n",
            "Epoch 15/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.3440 - val_loss: 2.7757 - learning_rate: 2.0000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2546 - val_loss: 2.8940 - learning_rate: 2.0000e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2296 - val_loss: 2.6895 - learning_rate: 2.0000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2158 - val_loss: 2.7304 - learning_rate: 4.0000e-06\n",
            "Epoch 19/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.0261 - val_loss: 2.7821 - learning_rate: 4.0000e-06\n",
            "Epoch 20/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.3167 - val_loss: 2.7406 - learning_rate: 4.0000e-06\n",
            "Epoch 21/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.0093 - val_loss: 2.7498 - learning_rate: 4.0000e-06\n",
            "Epoch 22/100\n",
            "\u001b[1m1332/1332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2486 - val_loss: 2.7587 - learning_rate: 8.0000e-07\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
            "Validation RMSE: 1.484710454940796\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXKyJCSAzqZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2_-LVZ8zqW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsBMb8-gzqUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyANctFoN7mx"
      },
      "source": [
        "# Base Lstm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load and process training data (adjust with your file paths)\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "# Define time-series and engineered columns\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction'\n",
        "]\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train, y_val = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(300, return_sequences=False))(seq_inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00005), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPCVKp6XdgX_",
        "outputId": "15c19969-9c42-43fa-e3d6-fecb19943acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 7ms/step - loss: 3.1538 - val_loss: 2.3952 - learning_rate: 5.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 2.3776 - val_loss: 2.3898 - learning_rate: 5.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 7ms/step - loss: 2.1626 - val_loss: 2.3598 - learning_rate: 5.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 7ms/step - loss: 2.4342 - val_loss: 2.4163 - learning_rate: 5.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 2.0585 - val_loss: 2.3966 - learning_rate: 5.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.8956 - val_loss: 2.4505 - learning_rate: 5.0000e-05\n",
            "Epoch 7/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 2.1572 - val_loss: 2.2283 - learning_rate: 5.0000e-05\n",
            "Epoch 8/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.8209 - val_loss: 2.2642 - learning_rate: 5.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.9860 - val_loss: 2.5594 - learning_rate: 5.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.8979 - val_loss: 2.4041 - learning_rate: 5.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 7ms/step - loss: 1.6802 - val_loss: 2.4777 - learning_rate: 5.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 7ms/step - loss: 1.8119 - val_loss: 2.4526 - learning_rate: 2.5000e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.5085 - val_loss: 2.4968 - learning_rate: 2.5000e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.6572 - val_loss: 2.4110 - learning_rate: 2.5000e-05\n",
            "Epoch 15/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.7214 - val_loss: 2.2976 - learning_rate: 2.5000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 7ms/step - loss: 1.4837 - val_loss: 2.4511 - learning_rate: 1.2500e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7ms/step - loss: 1.4189 - val_loss: 2.4417 - learning_rate: 1.2500e-05\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
            "Validation RMSE: 1.4741381406784058\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional, Input, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM or GRU model (with stronger regularization and batch normalization)\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng, use_gru=False):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "\n",
        "    if use_gru:\n",
        "        x = Bidirectional(GRU(300, return_sequences=False, kernel_regularizer=l1_l2(l1=0.001, l2=0.001)))(seq_inputs)  # GRU with L1 & L2 regularization\n",
        "    else:\n",
        "        x = Bidirectional(LSTM(300, return_sequences=False, kernel_regularizer=l1_l2(l1=0.001, l2=0.001)))(seq_inputs)  # LSTM with L1 & L2 regularization\n",
        "\n",
        "    x = BatchNormalization()(x)  # Batch normalization after LSTM/GRU\n",
        "    x = Dropout(0.5)(x)  # Increase dropout to 0.5\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(combined)\n",
        "    x = BatchNormalization()(x)  # Batch normalization after Dense\n",
        "    x = Dropout(0.5)(x)  # Increase dropout to 0.5\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model (with GRU option for comparison)\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng, use_gru=False)  # Switch to True if you want to use GRU\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)  # Further reduced patience\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=150,\n",
        "    batch_size=64,  # Increased batch size\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "vZKlCz2L5Zya",
        "outputId": "29dcfdd3-b60b-41f8-e041-6eab916158ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 9ms/step - loss: 10.5097 - val_loss: 8.1850 - learning_rate: 1.0000e-05\n",
            "Epoch 2/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 9ms/step - loss: 7.8536 - val_loss: 8.5403 - learning_rate: 1.0000e-05\n",
            "Epoch 3/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 9ms/step - loss: 8.1881 - val_loss: 8.1344 - learning_rate: 1.0000e-05\n",
            "Epoch 4/150\n",
            "\u001b[1m10650/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 6.6487"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-2ef7a5f9a34f>\u001b[0m in \u001b[0;36m<cell line: 112>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_eng\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_eng\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGj_md10hIhU",
        "outputId": "38700178-b37f-4c29-aacc-535e0e04cf3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - loss: 5.0182 - val_loss: 2.7338 - learning_rate: 3.0000e-05\n",
            "Epoch 2/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 4.2428 - val_loss: 2.5363 - learning_rate: 3.0000e-05\n",
            "Epoch 3/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 8ms/step - loss: 3.1556 - val_loss: 2.6596 - learning_rate: 3.0000e-05\n",
            "Epoch 4/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 3.8051 - val_loss: 2.6438 - learning_rate: 3.0000e-05\n",
            "Epoch 5/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 8ms/step - loss: 3.4768 - val_loss: 2.7255 - learning_rate: 3.0000e-05\n",
            "Epoch 6/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 8ms/step - loss: 3.4464 - val_loss: 2.4303 - learning_rate: 3.0000e-05\n",
            "Epoch 7/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 8ms/step - loss: 3.1546 - val_loss: 2.4477 - learning_rate: 3.0000e-05\n",
            "Epoch 8/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 3.4804 - val_loss: 2.4951 - learning_rate: 3.0000e-05\n",
            "Epoch 9/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 4.1982 - val_loss: 2.4963 - learning_rate: 3.0000e-05\n",
            "Epoch 10/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 3.7454 - val_loss: 2.5862 - learning_rate: 3.0000e-05\n",
            "Epoch 11/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 3.1304 - val_loss: 2.5755 - learning_rate: 1.5000e-05\n",
            "Epoch 12/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 3.6170 - val_loss: 3.0895 - learning_rate: 1.5000e-05\n",
            "Epoch 13/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 4.1109 - val_loss: 3.3711 - learning_rate: 1.5000e-05\n",
            "Epoch 14/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 4.7725 - val_loss: 2.9654 - learning_rate: 1.5000e-05\n",
            "Epoch 15/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 3.7914 - val_loss: 2.7404 - learning_rate: 7.5000e-06\n",
            "Epoch 16/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9ms/step - loss: 3.9584 - val_loss: 2.6695 - learning_rate: 7.5000e-06\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step\n",
            "Validation RMSE: 1.5403152704238892\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the optimized LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(200, return_sequences=True))(seq_inputs)  # Stacked LSTM\n",
        "    x = LSTM(100, return_sequences=False)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    optimizer = RMSprop(learning_rate=0.00003, clipvalue=1.0)\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
        "\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=150,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import shap\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. SHAP Feature Importance (optional, can skip for training speed)\n",
        "def get_shap_importance(model, X_val_seq, X_val_eng):\n",
        "    X_val_combined = np.concatenate([X_val_seq.reshape(X_val_seq.shape[0], -1), X_val_eng], axis=1)\n",
        "    explainer = shap.KernelExplainer(lambda x: model.predict([x[:, :64].reshape(-1, 16, 4), x[:, 64:]]), X_val_combined)\n",
        "    shap_values = explainer.shap_values(X_val_combined, nsamples=100)\n",
        "    shap_mean_importance = np.abs(shap_values).mean(axis=0)\n",
        "    top_features = np.argsort(shap_mean_importance)[::-1][:20]  # Select top 20 features\n",
        "    return top_features\n",
        "\n",
        "# 5. Build the optimized LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(350, return_sequences=True))(seq_inputs)  # Increased to 350 units\n",
        "    x = LSTM(150, return_sequences=False)(x)  # Stacked LSTM layer with 150 units\n",
        "    x = Dropout(0.2)(x)  # Reduced Dropout to 0.2\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined)  # Increased dense layer\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    optimizer = Nadam(learning_rate=0.0001)  # Changed to Nadam optimizer\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 6. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=150,\n",
        "    batch_size=64,  # Increased batch size for smoother gradients\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 7. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arc7n8qIN2yU",
        "outputId": "fa0428a5-7482-4e88-8566-295401bc97d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 10ms/step - loss: 3.3191 - val_loss: 2.4092 - learning_rate: 1.0000e-04\n",
            "Epoch 2/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 10ms/step - loss: 2.4060 - val_loss: 2.2914 - learning_rate: 1.0000e-04\n",
            "Epoch 3/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 2.6685 - val_loss: 2.3246 - learning_rate: 1.0000e-04\n",
            "Epoch 4/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 2.5586 - val_loss: 2.2975 - learning_rate: 1.0000e-04\n",
            "Epoch 5/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 2.3241 - val_loss: 2.4082 - learning_rate: 1.0000e-04\n",
            "Epoch 6/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 2.0009 - val_loss: 2.2703 - learning_rate: 1.0000e-04\n",
            "Epoch 7/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 1.9588 - val_loss: 2.3879 - learning_rate: 1.0000e-04\n",
            "Epoch 8/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 2.0523 - val_loss: 2.3555 - learning_rate: 1.0000e-04\n",
            "Epoch 9/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 11ms/step - loss: 1.9774 - val_loss: 2.5499 - learning_rate: 1.0000e-04\n",
            "Epoch 10/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 11ms/step - loss: 2.1301 - val_loss: 2.4042 - learning_rate: 1.0000e-04\n",
            "Epoch 11/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 11ms/step - loss: 2.5878 - val_loss: 2.4815 - learning_rate: 1.0000e-04\n",
            "Epoch 12/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 11ms/step - loss: 1.8988 - val_loss: 2.5711 - learning_rate: 5.0000e-05\n",
            "Epoch 13/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 11ms/step - loss: 2.1078 - val_loss: 2.4800 - learning_rate: 5.0000e-05\n",
            "Epoch 14/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - loss: 1.7435 - val_loss: 2.6600 - learning_rate: 5.0000e-05\n",
            "Epoch 15/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 11ms/step - loss: 1.7073 - val_loss: 2.4346 - learning_rate: 5.0000e-05\n",
            "Epoch 16/150\n",
            "\u001b[1m10983/10983\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 11ms/step - loss: 1.6804 - val_loss: 2.6614 - learning_rate: 5.0000e-05\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step\n",
            "Validation RMSE: 1.4901902675628662\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import shap\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df.loc[:, f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df.loc[:, f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df.loc[:, f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df.loc[:, f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    df.loc[:, 'retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df.loc[:, 'retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df.loc[:, 'retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the optimized LSTM model\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(400, return_sequences=True, kernel_initializer='he_normal'))(seq_inputs)  # Increased LSTM units\n",
        "    x = BatchNormalization()(x)  # Added BatchNormalization\n",
        "    x = LSTM(200, return_sequences=False)(x)  # Stacked LSTM layer with 200 units\n",
        "    x = Dropout(0.25)(x)  # Adjusted Dropout\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined)  # Increased dense layer\n",
        "    x = BatchNormalization()(x)  # BatchNormalization to stabilize learning\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    optimizer = Nadam(learning_rate=0.00005)  # Slightly reduced learning rate\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
        "\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=200,\n",
        "    batch_size=128,  # Increased batch size to smooth gradients\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Generate Predictions for Test Data\n",
        "# ---------------------------------------\n",
        "\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],  # Replace 'ID' with actual column name in the test set\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc0MM-mLWb-1",
        "outputId": "519d3fb5-516b-4b07-d616-6bfffc72ab57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 12ms/step - loss: 4.6247 - val_loss: 4.4100 - learning_rate: 5.0000e-05\n",
            "Epoch 2/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 2.5980 - val_loss: 5.7143 - learning_rate: 5.0000e-05\n",
            "Epoch 3/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 2.3513 - val_loss: 5.1023 - learning_rate: 5.0000e-05\n",
            "Epoch 4/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 2.4631 - val_loss: 7.0263 - learning_rate: 5.0000e-05\n",
            "Epoch 5/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 2.3026 - val_loss: 5.5399 - learning_rate: 5.0000e-05\n",
            "Epoch 6/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 2.5715 - val_loss: 6.7586 - learning_rate: 2.5000e-05\n",
            "Epoch 7/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 2.3698 - val_loss: 6.6832 - learning_rate: 2.5000e-05\n",
            "Epoch 8/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 12ms/step - loss: 1.8977 - val_loss: 6.8285 - learning_rate: 2.5000e-05\n",
            "Epoch 9/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 12ms/step - loss: 2.0740 - val_loss: 9.5523 - learning_rate: 2.5000e-05\n",
            "Epoch 10/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 12ms/step - loss: 1.8652 - val_loss: 9.4668 - learning_rate: 1.2500e-05\n",
            "Epoch 11/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 12ms/step - loss: 1.9396 - val_loss: 10.3642 - learning_rate: 1.2500e-05\n",
            "Epoch 12/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 12ms/step - loss: 2.0572 - val_loss: 11.1237 - learning_rate: 1.2500e-05\n",
            "Epoch 13/200\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 12ms/step - loss: 1.7493 - val_loss: 15.9554 - learning_rate: 1.2500e-05\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step\n",
            "Validation RMSE: 2.047546625137329\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Input, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "# 1. Optimized Feature Engineering\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Convert boolean columns to numeric (if any boolean columns exist)\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        for column in columns:\n",
        "            if df[column].dtype == 'bool':\n",
        "                df[column] = df[column].astype(int)\n",
        "\n",
        "    # Aggregated and new features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df[f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df[f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df[f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df[f'{feature}_std'] = df[columns].std(axis=1)\n",
        "        df[f'{feature}_slope'] = (df[columns].iloc[:, -1] - df[columns].iloc[:, 0]) / 15\n",
        "        df[f'{feature}_acceleration'] = df[columns].diff().mean(axis=1)\n",
        "        df[f'{feature}_last_3_mean'] = df[columns].iloc[:, -3:].mean(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df['retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df['retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df['retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "    df['ad_iap_revenue_ratio'] = df['ad_revenue_total'] / (df['iap_revenue_total'] + 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "\n",
        "# Apply feature engineering\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "# Ensure no NaN values in numeric columns are filled with their mean\n",
        "numeric_columns = train_data.select_dtypes(include=[np.number]).columns\n",
        "train_data[numeric_columns] = train_data[numeric_columns].fillna(train_data[numeric_columns].mean())\n",
        "\n",
        "# For non-numeric columns, fill with empty strings\n",
        "non_numeric_columns = train_data.select_dtypes(exclude=[np.number]).columns\n",
        "train_data[non_numeric_columns] = train_data[non_numeric_columns].fillna('')\n",
        "\n",
        "# 3. Define engineered features and time series columns\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std', 'retention_slope', 'retention_acceleration', 'retention_last_3_mean',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std', 'ad_revenue_slope', 'ad_revenue_acceleration', 'ad_revenue_last_3_mean',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std', 'iap_revenue_slope', 'iap_revenue_acceleration', 'iap_revenue_last_3_mean',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std', 'level_advanced_slope', 'level_advanced_acceleration', 'level_advanced_last_3_mean',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction', 'ad_iap_revenue_ratio'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "# Convert relevant columns to float\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# 4. Apply scalers\n",
        "scaler_seq = RobustScaler()\n",
        "scaler_eng = RobustScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 5. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train, y_val = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 6. Optimized Model with Lower Complexity and Regularization\n",
        "def build_optimized_lstm(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "\n",
        "    # Reduce LSTM units and switch to GRU (more efficient)\n",
        "    x = GRU(128, return_sequences=True)(seq_inputs)\n",
        "    x = GRU(64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)  # Increase dropout\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "    y = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(eng_inputs)  # Add L2 regularization\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Dropout(0.4)(y)  # Increase dropout\n",
        "\n",
        "    combined = Concatenate()([x, y])\n",
        "\n",
        "    z = Dense(64, kernel_regularizer=l2(0.001))(combined)  # Add L2 regularization\n",
        "    z = LeakyReLU()(z)\n",
        "    z = BatchNormalization()(z)\n",
        "    z = Dropout(0.5)(z)  # Increase dropout\n",
        "\n",
        "    z = Dense(32, kernel_regularizer=l2(0.001))(z)  # Add L2 regularization\n",
        "    z = LeakyReLU()(z)\n",
        "    z = BatchNormalization()(z)\n",
        "    z = Dropout(0.4)(z)\n",
        "\n",
        "    outputs = Dense(1)(z)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 7. Build and train the model with early stopping and learning rate scheduler\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_optimized_lstm(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  # Reduce patience\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the model with a larger batch size\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val),\n",
        "    epochs=200,\n",
        "    batch_size=256,  # Increase batch size to speed up training\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8. Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# 9. Generate Predictions for Test Data\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'TARGET': test_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hLXaJuDdgUf",
        "outputId": "cd7cb703-7931-4c8a-ff9d-1679e56899ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 7.5254 - val_loss: 2.9538 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 5.6580 - val_loss: 3.0674 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 4.7978 - val_loss: 2.8781 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 4.7640 - val_loss: 2.5894 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 3.9376 - val_loss: 2.8889 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 4.0088 - val_loss: 2.7340 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 4.0118 - val_loss: 2.8707 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 4.7902 - val_loss: 2.9027 - learning_rate: 5.0000e-05\n",
            "Epoch 9/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 11ms/step - loss: 4.3451 - val_loss: 3.2602 - learning_rate: 5.0000e-05\n",
            "Epoch 10/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 11ms/step - loss: 3.5163 - val_loss: 3.1384 - learning_rate: 5.0000e-05\n",
            "Epoch 11/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 11ms/step - loss: 3.4290 - val_loss: 3.1796 - learning_rate: 2.5000e-05\n",
            "Epoch 12/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 3.7469 - val_loss: 3.1762 - learning_rate: 2.5000e-05\n",
            "Epoch 13/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 3.8414 - val_loss: 4.0441 - learning_rate: 2.5000e-05\n",
            "Epoch 14/200\n",
            "\u001b[1m2746/2746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 3.7362 - val_loss: 3.7088 - learning_rate: 1.2500e-05\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
            "Validation RMSE: 1.568185567855835\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Feature Engineering: Aggregate features and interactions\n",
        "def feature_engineering(df):\n",
        "    time_series_columns = {\n",
        "        'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "        'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "        'iap_revenue': [f'IAPRevenueD{i}' for i in range(16)],\n",
        "        'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    }\n",
        "\n",
        "    # Aggregate features\n",
        "    for feature, columns in time_series_columns.items():\n",
        "        df[f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "        df[f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "        df[f'{feature}_max'] = df[columns].max(axis=1)\n",
        "        df[f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "    # Interaction features\n",
        "    df['retention_ad_revenue_interaction'] = df['retention_total'] * df['ad_revenue_total']\n",
        "    df['retention_iap_revenue_interaction'] = df['retention_total'] * df['iap_revenue_total']\n",
        "    df['retention_level_interaction'] = df['retention_total'] * df['level_advanced_total']\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Load, merge, and process training data\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "train_data = feature_engineering(train_data)\n",
        "\n",
        "engineered_columns = [\n",
        "    'retention_total', 'retention_mean', 'retention_max', 'retention_std',\n",
        "    'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_max', 'ad_revenue_std',\n",
        "    'iap_revenue_total', 'iap_revenue_mean', 'iap_revenue_max', 'iap_revenue_std',\n",
        "    'level_advanced_total', 'level_advanced_mean', 'level_advanced_max', 'level_advanced_std',\n",
        "    'retention_ad_revenue_interaction', 'retention_iap_revenue_interaction', 'retention_level_interaction'\n",
        "]\n",
        "\n",
        "time_series_columns = [\n",
        "    *[f'RetentionD{i}' for i in range(16)],\n",
        "    *[f'AdRevenueD{i}' for i in range(16)],\n",
        "    *[f'IAPRevenueD{i}' for i in range(16)],\n",
        "    *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "]\n",
        "\n",
        "train_data[time_series_columns] = train_data[time_series_columns].astype(float)\n",
        "\n",
        "# Apply different scalers to sequential and engineered features\n",
        "scaler_seq = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "# Sequential data scaling\n",
        "X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], -1)\n",
        "X_sequential = scaler_seq.fit_transform(X_sequential).reshape(train_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features scaling\n",
        "X_engineered = scaler_eng.fit_transform(train_data[engineered_columns].values)\n",
        "\n",
        "y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_sequential, X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Build the LSTM model (300 units in Bidirectional LSTM)\n",
        "def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "    seq_inputs = Input(shape=input_shape_seq)\n",
        "    x = Bidirectional(LSTM(300, return_sequences=False))(seq_inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "    combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00005), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Build and train the model\n",
        "input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "input_shape_eng = (X_train_eng.shape[1],)\n",
        "model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_eng], y_train_seq,\n",
        "    validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Predict and calculate RMSE\n",
        "y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "print(f'Validation RMSE: {rmse}')\n",
        "\n",
        "# 7. Generate Predictions for Test Data\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "test_data = feature_engineering(test_data)\n",
        "\n",
        "# Sequential data for the test set\n",
        "X_test_seq = test_data[time_series_columns].values.reshape(test_data.shape[0], -1)\n",
        "X_test_seq = scaler_seq.transform(X_test_seq).reshape(test_data.shape[0], 16, 4)\n",
        "\n",
        "# Engineered features for the test set\n",
        "X_test_eng = scaler_eng.transform(test_data[engineered_columns].values)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict([X_test_seq, X_test_eng])\n",
        "\n",
        "# Ensure predictions are numeric and replace any invalid values\n",
        "test_predictions = np.nan_to_num(test_predictions).flatten()  # Replace NaN with 0 or a small value if needed\n",
        "\n",
        "# Create the submission file with proper formatting\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'TARGET': test_predictions.astype(float)  # Ensure target values are floats\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsFXax_AdgPO",
        "outputId": "8d90a7e1-2e5b-4a20-c0cf-5e8803e7fbb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 7ms/step - loss: 3.5191 - val_loss: 2.5590 - learning_rate: 5.0000e-05\n",
            "Epoch 2/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 2.6463 - val_loss: 2.4640 - learning_rate: 5.0000e-05\n",
            "Epoch 3/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 2.3498 - val_loss: 2.3848 - learning_rate: 5.0000e-05\n",
            "Epoch 4/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 2.1368 - val_loss: 2.3248 - learning_rate: 5.0000e-05\n",
            "Epoch 5/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 2.1145 - val_loss: 2.2588 - learning_rate: 5.0000e-05\n",
            "Epoch 6/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 2.4595 - val_loss: 2.3130 - learning_rate: 5.0000e-05\n",
            "Epoch 7/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 2.0506 - val_loss: 2.4139 - learning_rate: 5.0000e-05\n",
            "Epoch 8/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 1.9860 - val_loss: 2.3398 - learning_rate: 5.0000e-05\n",
            "Epoch 9/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.8059 - val_loss: 2.1114 - learning_rate: 5.0000e-05\n",
            "Epoch 10/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.7861 - val_loss: 2.5822 - learning_rate: 5.0000e-05\n",
            "Epoch 11/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.8079 - val_loss: 2.3557 - learning_rate: 5.0000e-05\n",
            "Epoch 12/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.6955 - val_loss: 2.5169 - learning_rate: 5.0000e-05\n",
            "Epoch 13/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.4196 - val_loss: 2.4022 - learning_rate: 5.0000e-05\n",
            "Epoch 14/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 1.3593 - val_loss: 2.4712 - learning_rate: 2.5000e-05\n",
            "Epoch 15/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.4267 - val_loss: 2.2141 - learning_rate: 2.5000e-05\n",
            "Epoch 16/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.4999 - val_loss: 2.3140 - learning_rate: 2.5000e-05\n",
            "Epoch 17/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.3851 - val_loss: 2.2842 - learning_rate: 2.5000e-05\n",
            "Epoch 18/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.4452 - val_loss: 2.3561 - learning_rate: 1.2500e-05\n",
            "Epoch 19/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.4304 - val_loss: 2.3467 - learning_rate: 1.2500e-05\n",
            "Epoch 20/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 1.2055 - val_loss: 2.3810 - learning_rate: 1.2500e-05\n",
            "Epoch 21/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.4167 - val_loss: 2.3803 - learning_rate: 1.2500e-05\n",
            "Epoch 22/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 1.4501 - val_loss: 2.3519 - learning_rate: 6.2500e-06\n",
            "Epoch 23/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7ms/step - loss: 1.2456 - val_loss: 2.4216 - learning_rate: 6.2500e-06\n",
            "Epoch 24/150\n",
            "\u001b[1m21965/21965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 7ms/step - loss: 1.7528 - val_loss: 2.4784 - learning_rate: 6.2500e-06\n",
            "\u001b[1m5492/5492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
            "Validation RMSE: 1.436509370803833\n",
            "\u001b[1m18305/18305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtMZ1gzq5Zu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csQw0vGM5Zs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4lN4tC65Zqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8c4Cwyz5Zod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqZrzajs5ZmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChI61xIqNyjC"
      },
      "source": [
        "# Quantile transform and feature selecting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMxQ-OCvNjKd"
      },
      "source": [
        "## Hyper parameters with optuna for lstm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHnxCtxQOpfx",
        "outputId": "10d92e8b-011a-4b30-cd25-a7adebb6678d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.34)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Imtg5DKnKP"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "# Objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Hyperparameters to be optimized\n",
        "    lstm_units = trial.suggest_int('lstm_units', 64, 256)  # LSTM units\n",
        "    dense_units = trial.suggest_int('dense_units', 32, 128)  # Dense units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)  # Dropout rate\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])  # Batch size\n",
        "\n",
        "    # Optimizer: Let Optuna choose from RMSprop, Adam, and SGD\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['RMSprop', 'Adam', 'SGD'])\n",
        "\n",
        "    # Learning rate: Optimized learning rate for selected optimizer\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Log scale for learning rate\n",
        "\n",
        "    # Choose the optimizer based on the Optuna trial\n",
        "    if optimizer_name == 'RMSprop':\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'Adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)  # Using momentum with SGD\n",
        "\n",
        "    # Build the LSTM model\n",
        "    def build_lstm_nn(input_shape_seq, input_shape_eng):\n",
        "        seq_inputs = Input(shape=input_shape_seq)\n",
        "        x = Bidirectional(LSTM(lstm_units, return_sequences=False))(seq_inputs)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        # Feature-engineered part\n",
        "        eng_inputs = Input(shape=input_shape_eng)\n",
        "\n",
        "        # Combine sequential and engineered parts\n",
        "        combined = Concatenate()([x, eng_inputs])\n",
        "\n",
        "        # Dense layers with regularization\n",
        "        x = Dense(dense_units, activation='relu')(combined)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = Dense(dense_units // 2, activation='relu')(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        # Output layer for regression\n",
        "        outputs = Dense(1)(x)\n",
        "\n",
        "        model = Model(inputs=[seq_inputs, eng_inputs], outputs=outputs)\n",
        "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def feature_engineering(df):\n",
        "    # Sequential features: keeping only relevant features from RFE\n",
        "      time_series_columns = {\n",
        "          'retention': [f'RetentionD{i}' for i in range(16)],\n",
        "          'ad_revenue': [f'AdRevenueD{i}' for i in range(16)],\n",
        "          'level_advanced': [f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "      }\n",
        "\n",
        "      # Aggregate features: sum, mean, and std\n",
        "      for feature, columns in time_series_columns.items():\n",
        "          # Convert boolean columns to integers if needed\n",
        "          df[columns] = df[columns].astype(float)  # Ensure all columns are numeric\n",
        "\n",
        "          df[f'{feature}_total'] = df[columns].sum(axis=1)\n",
        "          df[f'{feature}_mean'] = df[columns].mean(axis=1)\n",
        "          df[f'{feature}_std'] = df[columns].std(axis=1)\n",
        "\n",
        "          # Lag features: Difference between consecutive days\n",
        "          for i in range(1, 16):\n",
        "              df.loc[:, f'{feature}_lag_{i}'] = df[f'{columns[i]}'] - df[f'{columns[i - 1]}']\n",
        "\n",
        "\n",
        "          # Rolling statistics\n",
        "          df[f'{feature}_rolling_mean_5'] = df[columns].rolling(window=5, axis=1).mean().mean(axis=1)\n",
        "          df[f'{feature}_rolling_std_5'] = df[columns].rolling(window=5, axis=1).std().mean(axis=1)\n",
        "\n",
        "      # Create the new columns separately\n",
        "      new_columns = pd.DataFrame({\n",
        "          'retention_ad_revenue_interaction': df['retention_total'] * df['ad_revenue_total'],\n",
        "          'retention_level_interaction': df['retention_total'] * df['level_advanced_total']\n",
        "      })\n",
        "\n",
        "      # Concatenate the new columns with the original DataFrame\n",
        "      df = pd.concat([df, new_columns], axis=1)\n",
        "\n",
        "      return df\n",
        "# 2. Merge and process training data\n",
        "    train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "    train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "    train_data = feature_engineering(train_data)\n",
        "\n",
        "# List of the feature-engineered columns including new lag and rolling statistics\n",
        "    engineered_columns = [\n",
        "        'retention_total', 'retention_mean', 'retention_std',\n",
        "        'ad_revenue_total', 'ad_revenue_mean', 'ad_revenue_std',\n",
        "        'level_advanced_total', 'level_advanced_std',\n",
        "        'retention_ad_revenue_interaction', 'retention_level_interaction',\n",
        "        # Adding lag and rolling stats for each feature\n",
        "        'retention_lag_1', 'retention_lag_2', 'retention_lag_3', 'retention_lag_4', 'retention_lag_5',\n",
        "        'ad_revenue_lag_1', 'ad_revenue_lag_2', 'ad_revenue_lag_3', 'ad_revenue_lag_4', 'ad_revenue_lag_5',\n",
        "        'level_advanced_lag_1', 'level_advanced_lag_2', 'level_advanced_lag_3', 'level_advanced_lag_4', 'level_advanced_lag_5',\n",
        "        'retention_rolling_mean_5', 'retention_rolling_std_5',\n",
        "        'ad_revenue_rolling_mean_5', 'ad_revenue_rolling_std_5',\n",
        "        'level_advanced_rolling_mean_5', 'level_advanced_rolling_std_5'\n",
        "    ]\n",
        "\n",
        "    # Sequential columns for LSTM\n",
        "    time_series_columns = [\n",
        "        *[f'RetentionD{i}' for i in range(16)],\n",
        "        *[f'AdRevenueD{i}' for i in range(16)],\n",
        "        *[f'LevelAdvancedCountD{i}' for i in range(16)]\n",
        "    ]\n",
        "\n",
        "    # Convert boolean columns to integers\n",
        "    boolean_columns = train_data[time_series_columns].select_dtypes(include=['bool']).columns\n",
        "    train_data[boolean_columns] = train_data[boolean_columns].astype(int)\n",
        "\n",
        "    # Convert all inputs to float32\n",
        "    X_sequential = train_data[time_series_columns].values.reshape(train_data.shape[0], 16, 3).astype('float32')\n",
        "    X_engineered = train_data[engineered_columns].values.astype('float32')\n",
        "\n",
        "    # Apply quantile transformation to the selected engineered features\n",
        "    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
        "    X_engineered_transformed = qt.fit_transform(X_engineered)\n",
        "\n",
        "    y = train_data['TARGET'].values.astype('float32')\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    X_train_seq, X_val_seq, X_train_eng, X_val_eng, y_train_seq, y_val_seq = train_test_split(\n",
        "        X_sequential, X_engineered_transformed, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Build the model\n",
        "    input_shape_seq = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "    input_shape_eng = (X_train_eng.shape[1],)\n",
        "    model = build_lstm_nn(input_shape_seq, input_shape_eng)\n",
        "\n",
        "    # Early stopping and learning rate scheduler\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_train_seq, X_train_eng], y_train_seq,\n",
        "        validation_data=([X_val_seq, X_val_eng], y_val_seq),\n",
        "        epochs=100,\n",
        "        batch_size=batch_size,  # Use the batch size selected by Optuna\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        verbose=0  # Set verbose=0 for cleaner Optuna output\n",
        "    )\n",
        "\n",
        "    # Predict and calculate RMSE for validation set\n",
        "    y_val_pred = model.predict([X_val_seq, X_val_eng])\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_seq, y_val_pred))\n",
        "\n",
        "    return rmse\n",
        "\n",
        "# Create an Optuna study and optimize it\n",
        "study = optuna.create_study(direction='minimize')  # We want to minimize RMSE\n",
        "study.optimize(objective, n_trials=8)  # Run 30 trials of hyperparameter optimization\n",
        "\n",
        "# Output the best hyperparameters\n",
        "print(f\"Best trial: {study.best_trial.params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5L6yfppOFqh"
      },
      "source": [
        "# Hyper parametered lstm try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2BD16UCKnH-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGAnqex-KnGb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DYgQnvGKnDY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvluoRxeKnBy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I78qvvUxKm-t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnMIsuqIKm8Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjzPslU0Km6F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x1vAfKmKx5t",
        "outputId": "b162cbc7-1e99-42ea-f489-6e019a841e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final blended RMSE: 1.805513445119748\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import xgboost as xgb\n",
        "\n",
        "# Function to reduce memory usage\n",
        "def reduce_memory_usage(df):\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not isinstance(col_type, pd.CategoricalDtype):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "    return df\n",
        "\n",
        "# Merge user metadata, features, and targets\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "\n",
        "# Label encode categorical variables\n",
        "label_encoders = {}\n",
        "for column in train_data.select_dtypes(include=['category', 'object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column].astype(str))\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Feature engineering - Date-based features\n",
        "train_data['first_open_datetime'] = pd.to_datetime(train_data['first_open_timestamp'], unit='us')\n",
        "train_data['first_open_day'] = train_data['first_open_datetime'].dt.day\n",
        "train_data['first_open_month'] = train_data['first_open_datetime'].dt.month\n",
        "train_data['first_open_hour'] = train_data['first_open_datetime'].dt.hour\n",
        "\n",
        "# Aggregated behavioral features\n",
        "train_data['total_retention'] = train_data[[f'RetentionD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "train_data['total_levels_completed'] = train_data[[f'LevelAdvancedCountD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "train_data['total_ad_revenue'] = train_data[[f'AdRevenueD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "train_data['total_iap_revenue'] = train_data[[f'IAPRevenueD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "\n",
        "# Drop unnecessary columns (including datetime columns)\n",
        "useless_columns = ['ID', 'first_open_date', 'first_open_timestamp', 'local_first_open_timestamp', 'first_open_datetime']\n",
        "train_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = train_data.drop(columns=['TARGET'])\n",
        "y = train_data['TARGET']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical features\n",
        "cat_features = ['country', 'platform', 'device_category', 'device_brand', 'device_model', 'ad_network']\n",
        "\n",
        "# Scale the numerical features only\n",
        "num_features = [col for col in X_train.columns if col not in cat_features]\n",
        "scaler = StandardScaler()\n",
        "X_train_num_scaled = scaler.fit_transform(X_train[num_features])\n",
        "X_val_num_scaled = scaler.transform(X_val[num_features])\n",
        "\n",
        "# Combine scaled numerical features with original (unscaled) categorical features\n",
        "X_train_combined = np.hstack([X_train_num_scaled, X_train[cat_features].values])\n",
        "X_val_combined = np.hstack([X_val_num_scaled, X_val[cat_features].values])\n",
        "\n",
        "# Convert the data into CatBoost Pool format\n",
        "train_pool = Pool(X_train, y_train, cat_features=[X.columns.get_loc(col) for col in cat_features])\n",
        "val_pool = Pool(X_val, y_val, cat_features=[X.columns.get_loc(col) for col in cat_features])\n",
        "\n",
        "# Train CatBoost\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=1346,\n",
        "    learning_rate=0.042,\n",
        "    depth=8,\n",
        "    loss_function='RMSE',\n",
        "    silent=True\n",
        ")\n",
        "catboost_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=100)\n",
        "\n",
        "# XGBoost Best Parameters\n",
        "xgboost_model = xgb.XGBRegressor(\n",
        "    tree_method='hist',\n",
        "    subsample=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    reg_alpha=0.1,\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=1000,\n",
        "    min_child_weight=1,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.01,\n",
        "    gamma=0.1,\n",
        "    colsample_bytree=0.8,\n",
        "    device='cuda',\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Train XGBoost\n",
        "xgboost_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],  verbose=False)\n",
        "\n",
        "# Blend predictions\n",
        "catboost_preds = catboost_model.predict(X_val)\n",
        "xgboost_preds = xgboost_model.predict(X_val)\n",
        "final_preds = 0.5 * catboost_preds + 0.5 * xgboost_preds  # Blend with equal weights\n",
        "\n",
        "# Evaluate\n",
        "final_rmse = np.sqrt(mean_squared_error(y_val, final_preds))\n",
        "print(f'Final blended RMSE: {final_rmse}')\n",
        "\n",
        "# Test set predictions\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "for column in test_data.select_dtypes(include=['category', 'object']).columns:\n",
        "    le = label_encoders.get(column)\n",
        "    if le is not None:\n",
        "        test_data[column] = test_data[column].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
        "\n",
        "# Align test set columns with training set (handle any potential missing columns)\n",
        "test_data = test_data.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Make predictions on the test set using both models and blend them\n",
        "catboost_test_preds = catboost_model.predict(test_data)\n",
        "xgboost_test_preds = xgboost_model.predict(test_data)\n",
        "final_test_preds = 0.5 * catboost_test_preds + 0.5 * xgboost_test_preds\n",
        "\n",
        "# Create a submission file\n",
        "submission = pd.DataFrame({'ID': users_test['ID'], 'TARGET': final_test_preds})\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra1fFIHgok6F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y5aBSQ1EComB",
        "outputId": "f02ac10e-85f1-4459-a1fb-e934a34fbae4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py:1479: RuntimeWarning: overflow encountered in cast\n",
            "  return dtype.type(n)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py:731: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  the_mean = the_sum / count if count > 0 else np.nan\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
            "[I 2024-09-11 12:58:20,291] A new study created in memory with name: no-name-229b56d4-d5b6-4bd4-ab14-65d291e3c123\n",
            "[I 2024-09-11 12:59:07,198] Trial 0 finished with value: 1.6274292469024658 and parameters: {'n_estimators': 1094, 'learning_rate': 0.028416279861585833, 'max_depth': 10, 'subsample': 0.6752126557279976, 'colsample_bytree': 0.9307937929561753, 'reg_lambda': 6.2970212395971865, 'reg_alpha': 0.7968988460271819}. Best is trial 0 with value: 1.6274292469024658.\n",
            "[I 2024-09-11 12:59:34,192] Trial 1 finished with value: 1.617990255355835 and parameters: {'n_estimators': 887, 'learning_rate': 0.0339148668401466, 'max_depth': 6, 'subsample': 0.6801617613652958, 'colsample_bytree': 0.5520202914173351, 'reg_lambda': 8.879457324859349, 'reg_alpha': 7.550115168173278}. Best is trial 1 with value: 1.617990255355835.\n",
            "[I 2024-09-11 13:00:03,944] Trial 2 finished with value: 1.6086477041244507 and parameters: {'n_estimators': 562, 'learning_rate': 0.029768519815283747, 'max_depth': 10, 'subsample': 0.7053979319430691, 'colsample_bytree': 0.9568132293479523, 'reg_lambda': 0.571210278016823, 'reg_alpha': 0.7830305289626606}. Best is trial 2 with value: 1.6086477041244507.\n",
            "[I 2024-09-11 13:00:24,507] Trial 3 finished with value: 1.59964120388031 and parameters: {'n_estimators': 745, 'learning_rate': 0.04383208328001841, 'max_depth': 5, 'subsample': 0.9009808703581054, 'colsample_bytree': 0.517568412052036, 'reg_lambda': 1.168185084652462, 'reg_alpha': 7.198964713281805}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:01:06,726] Trial 4 finished with value: 1.6151682138442993 and parameters: {'n_estimators': 1319, 'learning_rate': 0.015716216502399922, 'max_depth': 8, 'subsample': 0.7650161697992697, 'colsample_bytree': 0.9495320320458213, 'reg_lambda': 3.4081596929475575, 'reg_alpha': 5.54807426272875}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:01:33,945] Trial 5 finished with value: 1.6017158031463623 and parameters: {'n_estimators': 1078, 'learning_rate': 0.019672785378458818, 'max_depth': 5, 'subsample': 0.6363297741900975, 'colsample_bytree': 0.8362596043459661, 'reg_lambda': 8.662533414357078, 'reg_alpha': 3.650523889591809}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:02:11,533] Trial 6 finished with value: 1.619214653968811 and parameters: {'n_estimators': 1234, 'learning_rate': 0.015518560583079984, 'max_depth': 7, 'subsample': 0.533272481850666, 'colsample_bytree': 0.550850789004689, 'reg_lambda': 6.7024414867530195, 'reg_alpha': 0.5942569005755775}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:02:34,631] Trial 7 finished with value: 1.6024348735809326 and parameters: {'n_estimators': 1381, 'learning_rate': 0.026709943405314913, 'max_depth': 4, 'subsample': 0.9241763809695216, 'colsample_bytree': 0.6861945586418795, 'reg_lambda': 1.1495758418434268, 'reg_alpha': 9.308172624769695}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:03:28,171] Trial 8 finished with value: 1.671107530593872 and parameters: {'n_estimators': 1401, 'learning_rate': 0.03883120110695791, 'max_depth': 12, 'subsample': 0.9015808817426209, 'colsample_bytree': 0.6910502884166503, 'reg_lambda': 8.842868322324282, 'reg_alpha': 8.230722059613882}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:04:07,497] Trial 9 finished with value: 1.6325714588165283 and parameters: {'n_estimators': 1228, 'learning_rate': 0.0026321611081670153, 'max_depth': 5, 'subsample': 0.8323808981151086, 'colsample_bytree': 0.7671124839855255, 'reg_lambda': 0.6166047039701419, 'reg_alpha': 8.394780985849147}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:04:28,814] Trial 10 finished with value: 1.6131954193115234 and parameters: {'n_estimators': 711, 'learning_rate': 0.04998931488284794, 'max_depth': 4, 'subsample': 0.9980804049594135, 'colsample_bytree': 0.5071908128188793, 'reg_lambda': 3.4496306273509143, 'reg_alpha': 5.329899768739514}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:04:49,763] Trial 11 finished with value: 1.6077749729156494 and parameters: {'n_estimators': 896, 'learning_rate': 0.04748680318765068, 'max_depth': 6, 'subsample': 0.5745633783162287, 'colsample_bytree': 0.8108682596010807, 'reg_lambda': 3.5873361630767366, 'reg_alpha': 3.1210184207441114}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:05:20,692] Trial 12 finished with value: 1.609693169593811 and parameters: {'n_estimators': 737, 'learning_rate': 0.01828085123341576, 'max_depth': 6, 'subsample': 0.5993542065130517, 'colsample_bytree': 0.8522049055589348, 'reg_lambda': 9.824093767797574, 'reg_alpha': 3.550860997144764}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:05:41,983] Trial 13 finished with value: 1.602771520614624 and parameters: {'n_estimators': 1052, 'learning_rate': 0.04114875027119314, 'max_depth': 4, 'subsample': 0.7791561029316638, 'colsample_bytree': 0.6607232558141645, 'reg_lambda': 5.704866199630928, 'reg_alpha': 6.245914728414426}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:06:15,261] Trial 14 finished with value: 1.6857696771621704 and parameters: {'n_estimators': 508, 'learning_rate': 0.007665129906237227, 'max_depth': 8, 'subsample': 0.8527649217230068, 'colsample_bytree': 0.8590902923408278, 'reg_lambda': 7.574792928866451, 'reg_alpha': 3.414187632267948}. Best is trial 3 with value: 1.59964120388031.\n",
            "[I 2024-09-11 13:06:40,836] Trial 15 finished with value: 1.5976413488388062 and parameters: {'n_estimators': 866, 'learning_rate': 0.021644813998937654, 'max_depth': 5, 'subsample': 0.6401384308877579, 'colsample_bytree': 0.6080372724671419, 'reg_lambda': 2.428082897898209, 'reg_alpha': 6.817797178753302}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[I 2024-09-11 13:07:10,351] Trial 16 finished with value: 1.6313693523406982 and parameters: {'n_estimators': 731, 'learning_rate': 0.036139554080284644, 'max_depth': 8, 'subsample': 0.9945258493592234, 'colsample_bytree': 0.6162505482109859, 'reg_lambda': 2.0795799588132398, 'reg_alpha': 6.789887370408276}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[I 2024-09-11 13:07:34,729] Trial 17 finished with value: 1.6012237071990967 and parameters: {'n_estimators': 940, 'learning_rate': 0.02293380286269271, 'max_depth': 5, 'subsample': 0.8157727987324856, 'colsample_bytree': 0.6017680610922544, 'reg_lambda': 2.201267583855327, 'reg_alpha': 9.686107951813788}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[I 2024-09-11 13:08:10,483] Trial 18 finished with value: 1.6449687480926514 and parameters: {'n_estimators': 650, 'learning_rate': 0.04493891817857438, 'max_depth': 10, 'subsample': 0.737209671833619, 'colsample_bytree': 0.5410157007610005, 'reg_lambda': 4.439275935838759, 'reg_alpha': 7.27364030314021}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[I 2024-09-11 13:08:47,687] Trial 19 finished with value: 1.6156384944915771 and parameters: {'n_estimators': 821, 'learning_rate': 0.011570307443117192, 'max_depth': 7, 'subsample': 0.9197182409921912, 'colsample_bytree': 0.6266845812407953, 'reg_lambda': 2.046478694080056, 'reg_alpha': 4.557130835574712}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[I 2024-09-11 13:09:11,455] Trial 20 finished with value: 1.6190030574798584 and parameters: {'n_estimators': 810, 'learning_rate': 0.031954632198278356, 'max_depth': 7, 'subsample': 0.5224592165970722, 'colsample_bytree': 0.7275680261825652, 'reg_lambda': 0.19019568936571107, 'reg_alpha': 8.636186003984188}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[I 2024-09-11 13:09:35,412] Trial 21 finished with value: 1.605104684829712 and parameters: {'n_estimators': 968, 'learning_rate': 0.02411027697131396, 'max_depth': 5, 'subsample': 0.8640360052222664, 'colsample_bytree': 0.5961642911035555, 'reg_lambda': 1.9996970178802806, 'reg_alpha': 9.828706814039027}. Best is trial 15 with value: 1.5976413488388062.\n",
            "[W 2024-09-11 13:09:50,690] Trial 22 failed with parameters: {'n_estimators': 949, 'learning_rate': 0.02279159178391719, 'max_depth': 5, 'subsample': 0.80905032449822, 'colsample_bytree': 0.5071773178756989, 'reg_lambda': 2.6610957743096852, 'reg_alpha': 6.457109632850445} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-17-3b5f0c08192f>\", line 95, in objective\n",
            "    model.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1108, in fit\n",
            "    self._Booster = train(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/training.py\", line 181, in train\n",
            "    bst.update(dtrain, iteration=i, fobj=obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 2101, in update\n",
            "    _LIB.XGBoosterUpdateOneIter(\n",
            "KeyboardInterrupt\n",
            "[W 2024-09-11 13:09:50,692] Trial 22 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3b5f0c08192f>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Run the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Print best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3b5f0c08192f>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Train the model with early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         model.fit(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1109\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "# Function to reduce memory usage\n",
        "def reduce_memory_usage(df):\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not isinstance(col_type, pd.CategoricalDtype):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            if not isinstance(col_type, pd.CategoricalDtype):\n",
        "                df[col] = df[col].astype('category')\n",
        "    return df\n",
        "\n",
        "# Load datasets\n",
        "users_train = reduce_memory_usage(users_train)\n",
        "user_features_train = reduce_memory_usage(user_features_train)\n",
        "targets_train = reduce_memory_usage(targets_train)\n",
        "\n",
        "# Merge user metadata, features, and targets\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "useless_columns = ['first_open_date', 'first_open_timestamp', 'local_first_open_timestamp']\n",
        "train_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Target encode categorical variables\n",
        "encoder = TargetEncoder(cols=train_data.select_dtypes(include=['category']).columns)\n",
        "train_data = encoder.fit_transform(train_data, train_data['TARGET'])\n",
        "\n",
        "# Separate features and target\n",
        "X = train_data.drop(columns=['TARGET'])\n",
        "y = train_data['TARGET']\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Optuna optimization objective\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to optimize\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05),\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 20),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n",
        "        'random_state': 42,\n",
        "        'tree_method': 'hist',\n",
        "        \"device\" :\"cuda\",# Use GPU if available\n",
        "        'eval_metric': 'rmse'\n",
        "    }\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    # Cross-validation loop\n",
        "    for train_index, val_index in kf.split(X_scaled):\n",
        "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Model initialization\n",
        "        model = XGBRegressor(**params, early_stopping_rounds=50)\n",
        "\n",
        "        # Train the model with early stopping\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Predict on validation set\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Calculate RMSE for this fold\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        rmse_scores.append(rmse)\n",
        "\n",
        "    # Average RMSE over all folds\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    return avg_rmse\n",
        "\n",
        "# Run the optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print best parameters\n",
        "best_params = study.best_params\n",
        "print(f'Best parameters: {best_params}')\n",
        "\n",
        "# Train a final model with best parameters on the entire training set\n",
        "model = XGBRegressor(**best_params, early_stopping_rounds=50)\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# Test set processing\n",
        "users_test = reduce_memory_usage(users_test)\n",
        "user_features_test = reduce_memory_usage(user_features_test)\n",
        "\n",
        "# Merge test data\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "test_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Target encode test set\n",
        "test_data = encoder.transform(test_data)\n",
        "\n",
        "# Align test set columns with training set\n",
        "test_data = test_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "# Scale the test set\n",
        "test_data_scaled = scaler.transform(test_data)\n",
        "\n",
        "# Make predictions on test set using the tuned model\n",
        "test_preds = model.predict(test_data_scaled)\n",
        "\n",
        "# Create a submission file\n",
        "submission = pd.DataFrame({'ID': users_test['ID'], 'TARGET': test_preds})\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ6_H6O-OIYB",
        "outputId": "ef63f4c9-b8f1-4835-dc42-4ed9543f45fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dpoWiSUTIbIS",
        "outputId": "03cc6ba9-8c43-48cd-a934-76603756be99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-11 13:56:59,919] A new study created in memory with name: no-name-828baf87-6c71-4f05-a80c-1138587cada0\n",
            "[I 2024-09-11 13:57:38,877] Trial 0 finished with value: 0.40481151459575726 and parameters: {'n_estimators': 1495, 'learning_rate': 0.0068778966526878114, 'max_depth': 14, 'subsample': 0.9287179017599254, 'colsample_bytree': 0.6867226360612574, 'reg_lambda': 2.049985303257367, 'reg_alpha': 2.615738141383645, 'iterations': 1474, 'depth': 11, 'l2_leaf_reg': 4.252153887916706, 'random_strength': 0.9986113707482451}. Best is trial 0 with value: 0.40481151459575726.\n",
            "[I 2024-09-11 13:59:01,219] Trial 1 finished with value: 0.3834178857259302 and parameters: {'n_estimators': 1294, 'learning_rate': 0.039921662184369516, 'max_depth': 20, 'subsample': 0.6377323608851418, 'colsample_bytree': 0.9280951400314306, 'reg_lambda': 3.8486062323410826, 'reg_alpha': 0.23432951549530692, 'iterations': 1162, 'depth': 15, 'l2_leaf_reg': 2.664826393781268, 'random_strength': 1.7777301891616974}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:00:04,687] Trial 2 finished with value: 0.3946597014851806 and parameters: {'n_estimators': 505, 'learning_rate': 0.02724822265475508, 'max_depth': 12, 'subsample': 0.5021798944383227, 'colsample_bytree': 0.8928922194459381, 'reg_lambda': 0.9991091284356658, 'reg_alpha': 2.337255614421572, 'iterations': 1250, 'depth': 14, 'l2_leaf_reg': 0.41572214697284404, 'random_strength': 1.5867132151666477}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:00:28,505] Trial 3 finished with value: 0.4072244452626984 and parameters: {'n_estimators': 1415, 'learning_rate': 0.00873948453891628, 'max_depth': 7, 'subsample': 0.6784643299019676, 'colsample_bytree': 0.5811046472239854, 'reg_lambda': 2.5466147155957595, 'reg_alpha': 9.47046999812136, 'iterations': 1066, 'depth': 11, 'l2_leaf_reg': 6.482820914689094, 'random_strength': 1.6702542090724788}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:00:42,679] Trial 4 finished with value: 0.40077164815542454 and parameters: {'n_estimators': 855, 'learning_rate': 0.029100878216894437, 'max_depth': 10, 'subsample': 0.6616000284054908, 'colsample_bytree': 0.8934603588218946, 'reg_lambda': 1.3897182270686461, 'reg_alpha': 6.939867292310472, 'iterations': 1187, 'depth': 8, 'l2_leaf_reg': 8.031105368745964, 'random_strength': 1.7997148364975808}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:01:01,129] Trial 5 finished with value: 0.4000096535699572 and parameters: {'n_estimators': 629, 'learning_rate': 0.035073223598095705, 'max_depth': 13, 'subsample': 0.9339313140681391, 'colsample_bytree': 0.8482832651489713, 'reg_lambda': 9.52560587923213, 'reg_alpha': 6.84870420468068, 'iterations': 1009, 'depth': 9, 'l2_leaf_reg': 1.3392435969697494, 'random_strength': 1.3031223485510697}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:01:22,874] Trial 6 finished with value: 0.3946205346705026 and parameters: {'n_estimators': 855, 'learning_rate': 0.023799711225198063, 'max_depth': 20, 'subsample': 0.5824334086953077, 'colsample_bytree': 0.8225442447128612, 'reg_lambda': 3.934401517604302, 'reg_alpha': 0.8695571252627982, 'iterations': 1321, 'depth': 9, 'l2_leaf_reg': 7.298443930087539, 'random_strength': 1.1600440331142179}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:03:05,999] Trial 7 finished with value: 0.4039993346432208 and parameters: {'n_estimators': 505, 'learning_rate': 0.03710472571520335, 'max_depth': 8, 'subsample': 0.7677844586301564, 'colsample_bytree': 0.7228078952656861, 'reg_lambda': 2.3903857974225615, 'reg_alpha': 9.547610706028703, 'iterations': 1492, 'depth': 13, 'l2_leaf_reg': 1.6395665299897297, 'random_strength': 1.361585968021693}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:03:13,510] Trial 8 finished with value: 0.40934751589916757 and parameters: {'n_estimators': 1471, 'learning_rate': 0.014567563876866151, 'max_depth': 7, 'subsample': 0.7018967298148402, 'colsample_bytree': 0.798273200098355, 'reg_lambda': 0.25607374920603226, 'reg_alpha': 3.892803458888628, 'iterations': 667, 'depth': 6, 'l2_leaf_reg': 9.422416615829574, 'random_strength': 0.5730232369031321}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:03:28,235] Trial 9 finished with value: 0.4080372810974725 and parameters: {'n_estimators': 887, 'learning_rate': 0.03865015050048429, 'max_depth': 14, 'subsample': 0.5988168821991793, 'colsample_bytree': 0.8656844440513551, 'reg_lambda': 2.0262085333886435, 'reg_alpha': 2.0958376155531266, 'iterations': 568, 'depth': 10, 'l2_leaf_reg': 5.0112261284061095, 'random_strength': 1.5008506997863487}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:05:13,196] Trial 10 finished with value: 0.40660377080264504 and parameters: {'n_estimators': 1240, 'learning_rate': 0.04909667562237752, 'max_depth': 19, 'subsample': 0.8753999746009098, 'colsample_bytree': 0.9723100923359993, 'reg_lambda': 6.904865301097827, 'reg_alpha': 5.186597298110345, 'iterations': 806, 'depth': 16, 'l2_leaf_reg': 3.2781746494169193, 'random_strength': 1.968828165566581}. Best is trial 1 with value: 0.3834178857259302.\n",
            "[I 2024-09-11 14:05:22,633] Trial 11 finished with value: 0.37362447815497374 and parameters: {'n_estimators': 1126, 'learning_rate': 0.019130839284656524, 'max_depth': 20, 'subsample': 0.5404667206302288, 'colsample_bytree': 0.9960968919338987, 'reg_lambda': 5.059996358253325, 'reg_alpha': 0.3258110190103913, 'iterations': 1299, 'depth': 5, 'l2_leaf_reg': 6.0718598045826555, 'random_strength': 0.9851585200596467}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:05:29,438] Trial 12 finished with value: 0.3860903922024671 and parameters: {'n_estimators': 1147, 'learning_rate': 0.015362813961719136, 'max_depth': 17, 'subsample': 0.5170181592169096, 'colsample_bytree': 0.9998894940766031, 'reg_lambda': 5.65226758413611, 'reg_alpha': 0.7125468484065314, 'iterations': 843, 'depth': 4, 'l2_leaf_reg': 2.827875833554684, 'random_strength': 0.7707138908195172}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:05:40,398] Trial 13 finished with value: 0.3832352543804775 and parameters: {'n_estimators': 1180, 'learning_rate': 0.04378548060438586, 'max_depth': 17, 'subsample': 0.7711014187171021, 'colsample_bytree': 0.9557979040304009, 'reg_lambda': 4.653860731704834, 'reg_alpha': 0.733952341232879, 'iterations': 1339, 'depth': 6, 'l2_leaf_reg': 5.637488476068568, 'random_strength': 0.9819118141546536}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:05:47,009] Trial 14 finished with value: 0.3925725161769715 and parameters: {'n_estimators': 1054, 'learning_rate': 0.04977768690470244, 'max_depth': 17, 'subsample': 0.7963033340864714, 'colsample_bytree': 0.9985896993992377, 'reg_lambda': 7.093425005645704, 'reg_alpha': 4.115184510034557, 'iterations': 1351, 'depth': 4, 'l2_leaf_reg': 5.682431528425631, 'random_strength': 0.927065994878335}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:05:59,012] Trial 15 finished with value: 0.38044441744485435 and parameters: {'n_estimators': 1040, 'learning_rate': 0.01951518648437512, 'max_depth': 17, 'subsample': 0.8175050366409853, 'colsample_bytree': 0.632235818911896, 'reg_lambda': 5.547832749657881, 'reg_alpha': 0.015890970837608764, 'iterations': 1328, 'depth': 6, 'l2_leaf_reg': 8.039469987163606, 'random_strength': 1.0567366851889508}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:06:10,198] Trial 16 finished with value: 0.3877594377730982 and parameters: {'n_estimators': 998, 'learning_rate': 0.019697247342740476, 'max_depth': 18, 'subsample': 0.8405344821047467, 'colsample_bytree': 0.5456786968207012, 'reg_lambda': 6.817603895012928, 'reg_alpha': 0.0185426362521766, 'iterations': 1109, 'depth': 6, 'l2_leaf_reg': 9.585663352874652, 'random_strength': 0.7233504141309596}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:06:20,649] Trial 17 finished with value: 0.4051528109096344 and parameters: {'n_estimators': 999, 'learning_rate': 0.0012978061939150527, 'max_depth': 16, 'subsample': 0.9817935366349555, 'colsample_bytree': 0.5951803654982735, 'reg_lambda': 8.636872388331481, 'reg_alpha': 1.719815633507399, 'iterations': 913, 'depth': 7, 'l2_leaf_reg': 8.122990813644947, 'random_strength': 1.1408221266584146}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:06:26,193] Trial 18 finished with value: 0.38973787111063185 and parameters: {'n_estimators': 738, 'learning_rate': 0.02062802948280692, 'max_depth': 16, 'subsample': 0.7238399342280946, 'colsample_bytree': 0.6547965666904061, 'reg_lambda': 5.671574711292291, 'reg_alpha': 3.5637172812398146, 'iterations': 1405, 'depth': 4, 'l2_leaf_reg': 6.814738959368686, 'random_strength': 0.7940393113161458}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:06:34,477] Trial 19 finished with value: 0.3914565810234642 and parameters: {'n_estimators': 1307, 'learning_rate': 0.031928112619382665, 'max_depth': 19, 'subsample': 0.8313552377521397, 'colsample_bytree': 0.7613591152734045, 'reg_lambda': 8.023117854376478, 'reg_alpha': 6.244789077347406, 'iterations': 1248, 'depth': 5, 'l2_leaf_reg': 8.596662895742481, 'random_strength': 0.5788490968506635}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:06:46,165] Trial 20 finished with value: 0.39613135652218523 and parameters: {'n_estimators': 1049, 'learning_rate': 0.013628184689196715, 'max_depth': 11, 'subsample': 0.589669175525445, 'colsample_bytree': 0.6393964882528428, 'reg_lambda': 3.475081877332486, 'reg_alpha': 1.5737159570183732, 'iterations': 1236, 'depth': 7, 'l2_leaf_reg': 7.364663975101216, 'random_strength': 1.1196419829828053}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:06:56,868] Trial 21 finished with value: 0.38515564853459994 and parameters: {'n_estimators': 1158, 'learning_rate': 0.020569202588768095, 'max_depth': 15, 'subsample': 0.7538142020065732, 'colsample_bytree': 0.9412083792203878, 'reg_lambda': 4.806865626487037, 'reg_alpha': 1.2006932182650536, 'iterations': 1362, 'depth': 6, 'l2_leaf_reg': 5.935960001378465, 'random_strength': 0.9764601233229505}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:07:06,705] Trial 22 finished with value: 0.3853562864643453 and parameters: {'n_estimators': 1166, 'learning_rate': 0.045817497050186826, 'max_depth': 18, 'subsample': 0.8266950865160032, 'colsample_bytree': 0.9361577709270641, 'reg_lambda': 4.95534334233831, 'reg_alpha': 0.23877447357655954, 'iterations': 1437, 'depth': 5, 'l2_leaf_reg': 4.359576106408229, 'random_strength': 0.9418488973971765}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:07:18,999] Trial 23 finished with value: 0.3954767111754868 and parameters: {'n_estimators': 1099, 'learning_rate': 0.02359293021403309, 'max_depth': 18, 'subsample': 0.7750004386507175, 'colsample_bytree': 0.7530981717633141, 'reg_lambda': 5.913616516658669, 'reg_alpha': 2.8753076661335655, 'iterations': 1295, 'depth': 7, 'l2_leaf_reg': 5.60187750373715, 'random_strength': 1.4226922101201265}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:07:34,327] Trial 24 finished with value: 0.4057193610964417 and parameters: {'n_estimators': 949, 'learning_rate': 0.04316057897071268, 'max_depth': 20, 'subsample': 0.8923115166908565, 'colsample_bytree': 0.525516771701297, 'reg_lambda': 4.309972176743051, 'reg_alpha': 1.1766571334443596, 'iterations': 1131, 'depth': 8, 'l2_leaf_reg': 8.660224751109535, 'random_strength': 0.8365238369470853}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:07:44,856] Trial 25 finished with value: 0.3756482886554888 and parameters: {'n_estimators': 1243, 'learning_rate': 0.03289901153620012, 'max_depth': 16, 'subsample': 0.729376154421969, 'colsample_bytree': 0.6851574146439431, 'reg_lambda': 3.2648353950640545, 'reg_alpha': 0.011641717146232189, 'iterations': 1349, 'depth': 5, 'l2_leaf_reg': 4.804456711416522, 'random_strength': 1.1991148475310873}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:07:52,580] Trial 26 finished with value: 0.39420011043920794 and parameters: {'n_estimators': 1309, 'learning_rate': 0.031243504301499345, 'max_depth': 15, 'subsample': 0.5435458724101468, 'colsample_bytree': 0.7107011763769392, 'reg_lambda': 3.0993758157813485, 'reg_alpha': 3.2140306853467253, 'iterations': 1036, 'depth': 5, 'l2_leaf_reg': 4.2924931240201385, 'random_strength': 1.1849176416428}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:08:01,463] Trial 27 finished with value: 0.38442436235569677 and parameters: {'n_estimators': 1401, 'learning_rate': 0.017660918526297545, 'max_depth': 16, 'subsample': 0.7211665320417627, 'colsample_bytree': 0.6270911688338103, 'reg_lambda': 6.3515855672708135, 'reg_alpha': 8.663021651567854, 'iterations': 1409, 'depth': 5, 'l2_leaf_reg': 6.3433848145117695, 'random_strength': 1.2574084493356084}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:08:14,006] Trial 28 finished with value: 0.4020238395950172 and parameters: {'n_estimators': 1242, 'learning_rate': 0.010863793691105461, 'max_depth': 19, 'subsample': 0.6414164740996243, 'colsample_bytree': 0.6761890045513276, 'reg_lambda': 3.0221858937919643, 'reg_alpha': 1.8552418426753046, 'iterations': 912, 'depth': 8, 'l2_leaf_reg': 3.624670896893186, 'random_strength': 1.0631495606163208}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:08:43,626] Trial 29 finished with value: 0.4086023316802179 and parameters: {'n_estimators': 930, 'learning_rate': 0.0045989214800949176, 'max_depth': 14, 'subsample': 0.917292730064004, 'colsample_bytree': 0.6061283747912554, 'reg_lambda': 5.251773088463017, 'reg_alpha': 4.638827496947026, 'iterations': 1490, 'depth': 12, 'l2_leaf_reg': 4.863644954409454, 'random_strength': 1.0475991988720281}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:08:49,781] Trial 30 finished with value: 0.38147587447365483 and parameters: {'n_estimators': 762, 'learning_rate': 0.023858986349808937, 'max_depth': 13, 'subsample': 0.9981523597988355, 'colsample_bytree': 0.7089311021169478, 'reg_lambda': 4.270983534963664, 'reg_alpha': 2.5019744002998796, 'iterations': 1190, 'depth': 4, 'l2_leaf_reg': 7.251760510554815, 'random_strength': 0.6800512124440907}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:08:56,243] Trial 31 finished with value: 0.380014981825083 and parameters: {'n_estimators': 658, 'learning_rate': 0.02494121031805712, 'max_depth': 13, 'subsample': 0.9536760707949659, 'colsample_bytree': 0.7059367854228045, 'reg_lambda': 4.538723758453566, 'reg_alpha': 0.03537556920736644, 'iterations': 1200, 'depth': 4, 'l2_leaf_reg': 7.311783709079723, 'random_strength': 0.6792256667665011}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:09:05,467] Trial 32 finished with value: 0.38041424170075644 and parameters: {'n_estimators': 1085, 'learning_rate': 0.026828018756215287, 'max_depth': 10, 'subsample': 0.9749933940176334, 'colsample_bytree': 0.7864904772235071, 'reg_lambda': 3.7008953158512656, 'reg_alpha': 0.1074298343954964, 'iterations': 1295, 'depth': 5, 'l2_leaf_reg': 7.775332204172187, 'random_strength': 0.8557991892098304}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:09:13,878] Trial 33 finished with value: 0.38390167151844634 and parameters: {'n_estimators': 1098, 'learning_rate': 0.027208136785919698, 'max_depth': 9, 'subsample': 0.9458249071255047, 'colsample_bytree': 0.7769637959934932, 'reg_lambda': 3.262395119350838, 'reg_alpha': 0.5746771589719721, 'iterations': 1261, 'depth': 5, 'l2_leaf_reg': 5.088283644131248, 'random_strength': 0.8612959914770346}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:09:21,640] Trial 34 finished with value: 0.38343528043475833 and parameters: {'n_estimators': 1230, 'learning_rate': 0.03307804004475032, 'max_depth': 12, 'subsample': 0.9570029375395213, 'colsample_bytree': 0.6679189472281417, 'reg_lambda': 3.6611670989458833, 'reg_alpha': 1.2964634639333095, 'iterations': 1199, 'depth': 4, 'l2_leaf_reg': 6.647509676173142, 'random_strength': 0.6032287437646984}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:09:35,369] Trial 35 finished with value: 0.3810696517451633 and parameters: {'n_estimators': 1376, 'learning_rate': 0.029051577502491724, 'max_depth': 11, 'subsample': 0.8874620769093691, 'colsample_bytree': 0.8991565134672387, 'reg_lambda': 1.6992011496933521, 'reg_alpha': 0.011354979623164736, 'iterations': 1133, 'depth': 7, 'l2_leaf_reg': 8.865185628492744, 'random_strength': 0.8752267272458129}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:09:42,502] Trial 36 finished with value: 0.3813873498072614 and parameters: {'n_estimators': 626, 'learning_rate': 0.025991604041514557, 'max_depth': 10, 'subsample': 0.9677807450428481, 'colsample_bytree': 0.7299694155425965, 'reg_lambda': 2.509031710104982, 'reg_alpha': 0.6314644417795021, 'iterations': 1404, 'depth': 5, 'l2_leaf_reg': 7.685474249922554, 'random_strength': 0.6766087502737962}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:10:08,776] Trial 37 finished with value: 0.40732130707643355 and parameters: {'n_estimators': 1341, 'learning_rate': 0.029759416755763154, 'max_depth': 6, 'subsample': 0.8642540355499254, 'colsample_bytree': 0.7997600125922179, 'reg_lambda': 4.104058731406169, 'reg_alpha': 2.221704572591798, 'iterations': 1282, 'depth': 9, 'l2_leaf_reg': 6.259393922246609, 'random_strength': 0.5024660574471135}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:10:14,256] Trial 38 finished with value: 0.38774440368740803 and parameters: {'n_estimators': 779, 'learning_rate': 0.03514053891884795, 'max_depth': 12, 'subsample': 0.9273448496712561, 'colsample_bytree': 0.837391067519927, 'reg_lambda': 0.9314363369052359, 'reg_alpha': 8.120789643904233, 'iterations': 1221, 'depth': 4, 'l2_leaf_reg': 3.5923818633462163, 'random_strength': 1.2278914543312225}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:11:05,168] Trial 39 finished with value: 0.4004772888662876 and parameters: {'n_estimators': 561, 'learning_rate': 0.02313374091997713, 'max_depth': 11, 'subsample': 0.6743021948333942, 'colsample_bytree': 0.6935490847631187, 'reg_lambda': 2.8204944250472015, 'reg_alpha': 1.1365935606889612, 'iterations': 963, 'depth': 14, 'l2_leaf_reg': 6.926243918728391, 'random_strength': 0.73904980412995}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:11:35,185] Trial 40 finished with value: 0.38750903715571716 and parameters: {'n_estimators': 1486, 'learning_rate': 0.02765592113507587, 'max_depth': 9, 'subsample': 0.6321024454424299, 'colsample_bytree': 0.7379864513722909, 'reg_lambda': 3.60762068954996, 'reg_alpha': 0.48633143451868655, 'iterations': 1097, 'depth': 10, 'l2_leaf_reg': 5.178489909081346, 'random_strength': 1.3155718292488991}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:11:46,176] Trial 41 finished with value: 0.38370306637487633 and parameters: {'n_estimators': 1092, 'learning_rate': 0.018424967111714218, 'max_depth': 15, 'subsample': 0.9125181133342449, 'colsample_bytree': 0.5768638830274956, 'reg_lambda': 5.258716816027382, 'reg_alpha': 0.4087571847382098, 'iterations': 1324, 'depth': 6, 'l2_leaf_reg': 8.033228175537836, 'random_strength': 1.0657325893377945}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:11:58,385] Trial 42 finished with value: 0.3876362239143639 and parameters: {'n_estimators': 1051, 'learning_rate': 0.011571991912188058, 'max_depth': 14, 'subsample': 0.7994140907988851, 'colsample_bytree': 0.6872551710024928, 'reg_lambda': 6.105137995740541, 'reg_alpha': 0.03643547234138572, 'iterations': 1379, 'depth': 6, 'l2_leaf_reg': 9.967992449143363, 'random_strength': 0.9140779711269569}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:12:06,535] Trial 43 finished with value: 0.3830090092245327 and parameters: {'n_estimators': 941, 'learning_rate': 0.02177710092680429, 'max_depth': 13, 'subsample': 0.5495292232665852, 'colsample_bytree': 0.6300580576366195, 'reg_lambda': 4.500516956492671, 'reg_alpha': 0.9732409630011856, 'iterations': 1292, 'depth': 5, 'l2_leaf_reg': 7.760709653180818, 'random_strength': 1.4365075291319422}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:12:14,521] Trial 44 finished with value: 0.38130682116167 and parameters: {'n_estimators': 1212, 'learning_rate': 0.01734473520585135, 'max_depth': 20, 'subsample': 0.9853157417167857, 'colsample_bytree': 0.5648040771609342, 'reg_lambda': 5.3278311772282, 'reg_alpha': 1.451965300023733, 'iterations': 1147, 'depth': 4, 'l2_leaf_reg': 8.371815107849594, 'random_strength': 1.0299970297278798}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:12:24,518] Trial 45 finished with value: 0.3911537889569511 and parameters: {'n_estimators': 848, 'learning_rate': 0.02543478137124902, 'max_depth': 13, 'subsample': 0.8605232370561978, 'colsample_bytree': 0.6499421963602116, 'reg_lambda': 7.503086924356954, 'reg_alpha': 1.9684126234883734, 'iterations': 1439, 'depth': 6, 'l2_leaf_reg': 7.153234856776773, 'random_strength': 1.1305353865110341}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:12:38,381] Trial 46 finished with value: 0.39087597867075397 and parameters: {'n_estimators': 1135, 'learning_rate': 0.03463535724426584, 'max_depth': 17, 'subsample': 0.6972563812536529, 'colsample_bytree': 0.6147834393444527, 'reg_lambda': 3.9584471493292055, 'reg_alpha': 0.8042524959509303, 'iterations': 1320, 'depth': 7, 'l2_leaf_reg': 0.055613848075667605, 'random_strength': 1.6024910220087043}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:12:57,055] Trial 47 finished with value: 0.4059787853102545 and parameters: {'n_estimators': 1266, 'learning_rate': 0.03768042670556397, 'max_depth': 19, 'subsample': 0.6241229455444782, 'colsample_bytree': 0.8637839045548316, 'reg_lambda': 2.1935836623631206, 'reg_alpha': 2.884038862201594, 'iterations': 1459, 'depth': 8, 'l2_leaf_reg': 8.943252947301604, 'random_strength': 0.8428506118521181}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:13:05,854] Trial 48 finished with value: 0.3860859083964098 and parameters: {'n_estimators': 1035, 'learning_rate': 0.016073087940596392, 'max_depth': 8, 'subsample': 0.9030074556784082, 'colsample_bytree': 0.5062648074018954, 'reg_lambda': 6.556738801837882, 'reg_alpha': 0.3532276597698787, 'iterations': 1185, 'depth': 5, 'l2_leaf_reg': 6.077739183679166, 'random_strength': 0.6451972267362919}. Best is trial 11 with value: 0.37362447815497374.\n",
            "[I 2024-09-11 14:13:46,762] Trial 49 finished with value: 0.3767781816351829 and parameters: {'n_estimators': 984, 'learning_rate': 0.02967776498283905, 'max_depth': 16, 'subsample': 0.5606903984009253, 'colsample_bytree': 0.7890706462776385, 'reg_lambda': 9.958281667138783, 'reg_alpha': 0.01211340682019911, 'iterations': 1057, 'depth': 11, 'l2_leaf_reg': 1.8594591821881123, 'random_strength': 0.7823462646122711}. Best is trial 11 with value: 0.37362447815497374.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'n_estimators': 1126, 'learning_rate': 0.019130839284656524, 'max_depth': 20, 'subsample': 0.5404667206302288, 'colsample_bytree': 0.9960968919338987, 'reg_lambda': 5.059996358253325, 'reg_alpha': 0.3258110190103913, 'iterations': 1299, 'depth': 5, 'l2_leaf_reg': 6.0718598045826555, 'random_strength': 0.9851585200596467}\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "CatBoostRegressor.__init__() got an unexpected keyword argument 'colsample_bytree'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7efd994598d0>\u001b[0m in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m ]\n\u001b[1;32m    164\u001b[0m \u001b[0mmeta_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CatBoostRegressor.__init__() got an unexpected keyword argument 'colsample_bytree'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "# Function to reduce memory usage\n",
        "def reduce_memory_usage(df):\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not isinstance(col_type, pd.CategoricalDtype):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            if not isinstance(col_type, pd.CategoricalDtype):\n",
        "                df[col] = df[col].astype('category')\n",
        "    return df\n",
        "\n",
        "# Custom Model for Stacking\n",
        "class StackedModel(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, models, meta_model):\n",
        "        self.models = models\n",
        "        self.meta_model = meta_model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.fitted_models = [model.fit(X, y) for model in self.models]\n",
        "        meta_X = np.column_stack([model.predict(X) for model in self.fitted_models])\n",
        "        self.meta_model.fit(meta_X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_X = np.column_stack([model.predict(X) for model in self.fitted_models])\n",
        "        return self.meta_model.predict(meta_X)\n",
        "\n",
        "# Load datasets\n",
        "users_train = reduce_memory_usage(users_train)\n",
        "user_features_train = reduce_memory_usage(user_features_train)\n",
        "targets_train = reduce_memory_usage(targets_train)\n",
        "\n",
        "# Merge user metadata, features, and targets\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "\n",
        "train_data=train_data.head(50)\n",
        "# Drop unnecessary columns\n",
        "useless_columns = ['ID', 'first_open_date', 'first_open_timestamp', 'local_first_open_timestamp']\n",
        "train_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Target encode categorical variables\n",
        "encoder = TargetEncoder(cols=train_data.select_dtypes(include=['category']).columns)\n",
        "train_data = encoder.fit_transform(train_data, train_data['TARGET'])\n",
        "\n",
        "# Separate features and target\n",
        "X = train_data.drop(columns=['TARGET'])\n",
        "y = train_data['TARGET']\n",
        "\n",
        "# Apply log transformation to the target\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the models for the ensemble\n",
        "def objective(trial):\n",
        "    xgb_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05),\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 20),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n",
        "        'tree_method': 'hist',\n",
        "        \"device\":\"cuda\"\n",
        "    }\n",
        "\n",
        "    lgb_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05),\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 20),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n",
        "        \"verbose\" : -1\n",
        "    }\n",
        "\n",
        "    catboost_params = {\n",
        "        'iterations': trial.suggest_int('iterations', 500, 1500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05),\n",
        "        'depth': trial.suggest_int('depth', 4, 16),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.01, 10.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'random_strength': trial.suggest_float('random_strength', 0.5, 2.0)\n",
        "    }\n",
        "\n",
        "    models = [\n",
        "        XGBRegressor(**xgb_params),\n",
        "        LGBMRegressor(**lgb_params),\n",
        "        CatBoostRegressor(**catboost_params, silent=True)\n",
        "    ]\n",
        "\n",
        "    meta_model = RidgeCV()\n",
        "\n",
        "    stacked_model = StackedModel(models=models, meta_model=meta_model)\n",
        "\n",
        "    rmse_scores = []\n",
        "    for train_index, val_index in kf.split(X_scaled):\n",
        "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
        "        y_train, y_val = y_log[train_index], y_log[val_index]\n",
        "\n",
        "        stacked_model.fit(X_train, y_train)\n",
        "        y_pred_log = stacked_model.predict(X_val)\n",
        "        y_pred = np.expm1(y_pred_log)  # Reverse the log transformation\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(np.expm1(y_val), y_pred))\n",
        "        rmse_scores.append(rmse)\n",
        "\n",
        "    return np.mean(rmse_scores)\n",
        "\n",
        "# Run the optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print best parameters\n",
        "best_params = study.best_params\n",
        "print(f'Best parameters: {best_params}')\n",
        "\n",
        "# Train a final stacked model with best parameters\n",
        "models = [\n",
        "    XGBRegressor(**best_params),\n",
        "    LGBMRegressor(**best_params),\n",
        "    CatBoostRegressor(**best_params, silent=True)\n",
        "]\n",
        "meta_model = RidgeCV()\n",
        "stacked_model = StackedModel(models=models, meta_model=meta_model)\n",
        "stacked_model.fit(X_scaled, y_log)\n",
        "\n",
        "# Test set processing\n",
        "users_test = reduce_memory_usage(users_test)\n",
        "user_features_test = reduce_memory_usage(user_features_test)\n",
        "\n",
        "# Merge test data\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "\n",
        "test_data=test_data.head(50)\n",
        "# Drop unnecessary columns\n",
        "test_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Target encode test set\n",
        "test_data = encoder.transform(test_data)\n",
        "\n",
        "# Align test set columns with training set\n",
        "test_data = test_data.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "# Scale the test set\n",
        "test_data_scaled = scaler.transform(test_data)\n",
        "\n",
        "# Make predictions on test set using the stacked model\n",
        "test_preds_log = stacked_model.predict(test_data_scaled)\n",
        "test_preds = np.expm1(test_preds_log)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqPIS2CTOFGH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMmuFDhCe1lp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab6Q1xJBe1ih",
        "outputId": "472dcc53-fff9-4ca7-80b1-23318c293893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final RMSE on the training set: 1.4228414773605031\n",
            "Final RMSE on the validation set: 1.8227595465578872\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Function to reduce memory usage\n",
        "def reduce_memory_usage(df):\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not isinstance(col_type, pd.CategoricalDtype):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            if not isinstance(col_type, pd.CategoricalDtype):\n",
        "                df[col] = df[col].astype('category')\n",
        "    return df\n",
        "\n",
        "# Load datasets\n",
        "users_train = reduce_memory_usage(pd.read_csv('users_train.csv'))\n",
        "user_features_train = reduce_memory_usage(pd.read_csv('user_features_train.csv'))\n",
        "targets_train = reduce_memory_usage(pd.read_csv('targets_train.csv'))\n",
        "\n",
        "# Merge user metadata, features, and targets\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "useless_columns = ['ID', 'first_open_date', 'first_open_timestamp', 'local_first_open_timestamp']\n",
        "train_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Label encode categorical variables\n",
        "label_encoders = {}\n",
        "for column in train_data.select_dtypes(include=['category', 'object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column].astype(str))\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Separate features and target\n",
        "X = train_data.drop(columns=['TARGET'])\n",
        "y = train_data['TARGET']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply log transformation to the target\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_val_log = np.log1p(y_val)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Best parameters from Optuna tuning\n",
        "best_params = {\n",
        "    'bootstrap_type': 'Bayesian',\n",
        "    'iterations': 1346,\n",
        "    'learning_rate': 0.04412581031554557,\n",
        "    'depth': 4,\n",
        "    'l2_leaf_reg': 4.814450377788459,\n",
        "    'random_strength': 0.9047553220374154,\n",
        "    'silent': True  # To suppress training logs\n",
        "}\n",
        "\n",
        "# Train final CatBoost model with the best parameters\n",
        "best_catboost_model = CatBoostRegressor(**best_params)\n",
        "best_catboost_model.fit(X_train_scaled, y_train_log)\n",
        "\n",
        "# Calculate RMSE on the training set\n",
        "train_preds_log = best_catboost_model.predict(X_train_scaled)\n",
        "train_preds = np.expm1(train_preds_log)\n",
        "train_rmse = np.sqrt(mean_squared_error(np.expm1(y_train_log), train_preds))\n",
        "print(f'Final RMSE on the training set: {train_rmse}')\n",
        "\n",
        "# Calculate RMSE on the validation set\n",
        "val_preds_log = best_catboost_model.predict(X_val_scaled)\n",
        "val_preds = np.expm1(val_preds_log)\n",
        "val_rmse = np.sqrt(mean_squared_error(np.expm1(y_val_log), val_preds))\n",
        "print(f'Final RMSE on the validation set: {val_rmse}')\n",
        "\n",
        "# Test set processing\n",
        "users_test = reduce_memory_usage(pd.read_csv('users_test.csv'))\n",
        "user_features_test = reduce_memory_usage(pd.read_csv('user_features_test.csv'))\n",
        "\n",
        "# Merge test data\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "test_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Apply label encoding to the test set using the same encoders as in the training set\n",
        "for column in test_data.select_dtypes(include=['category', 'object']).columns:\n",
        "    le = label_encoders.get(column)\n",
        "    if le is not None:\n",
        "        # Handle unseen labels by mapping them to a default value (-1)\n",
        "        test_data[column] = test_data[column].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
        "\n",
        "# Align test set columns with training set (handle any potential missing columns)\n",
        "test_data = test_data.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Scale the test set\n",
        "test_data_scaled = scaler.transform(test_data)\n",
        "\n",
        "# Make predictions on the test set using the final CatBoost model\n",
        "test_preds_log = best_catboost_model.predict(test_data_scaled)\n",
        "test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "# Create a submission file\n",
        "submission = pd.DataFrame({'ID': users_test['ID'], 'TARGET': test_preds})\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqexbR4se2MB",
        "outputId": "21b15f31-4aa5-4803-8c54-c90ba1c98d35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-11 16:27:46,662] A new study created in memory with name: no-name-c4d259fe-b5f2-4376-a96b-cb46ea31d9db\n",
            "[I 2024-09-11 16:28:20,166] Trial 0 finished with value: 1.8133966572437532 and parameters: {'iterations': 1076, 'learning_rate': 0.06279732410030457, 'depth': 5, 'l2_leaf_reg': 9.278194532222534, 'random_strength': 1.4302327096762606, 'bootstrap_type': 'MVS', 'grow_policy': 'SymmetricTree', 'subsample': 0.8359788010248591}. Best is trial 0 with value: 1.8133966572437532.\n",
            "[I 2024-09-11 16:28:43,456] Trial 1 finished with value: 1.8968406723916555 and parameters: {'iterations': 1069, 'learning_rate': 0.014503924999202427, 'depth': 3, 'l2_leaf_reg': 1.1095666603169931, 'random_strength': 1.4270950861816796, 'bootstrap_type': 'MVS', 'grow_policy': 'SymmetricTree', 'subsample': 0.5647331311665895}. Best is trial 0 with value: 1.8133966572437532.\n",
            "[I 2024-09-11 16:31:51,968] Trial 2 finished with value: 1.742345530997049 and parameters: {'iterations': 875, 'learning_rate': 0.09057717158513187, 'depth': 6, 'l2_leaf_reg': 3.7320099553482753, 'random_strength': 1.6557146322683698, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9957136834961512}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:34:17,889] Trial 3 finished with value: 1.7807639641539166 and parameters: {'iterations': 1958, 'learning_rate': 0.0847255853943313, 'depth': 6, 'l2_leaf_reg': 3.7845621833944825, 'random_strength': 0.9616329174648677, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Depthwise'}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:36:36,538] Trial 4 finished with value: 1.7828396944061262 and parameters: {'iterations': 1260, 'learning_rate': 0.05837352385146808, 'depth': 8, 'l2_leaf_reg': 3.173091427897442, 'random_strength': 1.1375331616751387, 'bootstrap_type': 'MVS', 'grow_policy': 'Depthwise', 'subsample': 0.9065283904366145}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:37:33,827] Trial 5 finished with value: 1.8279767923308092 and parameters: {'iterations': 1706, 'learning_rate': 0.09766455824747383, 'depth': 7, 'l2_leaf_reg': 4.34823534605528, 'random_strength': 1.9994738890769868, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'subsample': 0.606764209410019}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:43:06,040] Trial 6 finished with value: 1.936032482202458 and parameters: {'iterations': 1584, 'learning_rate': 0.054857914917100854, 'depth': 10, 'l2_leaf_reg': 0.07618971971066264, 'random_strength': 1.2484476093103454, 'bootstrap_type': 'MVS', 'grow_policy': 'Depthwise', 'subsample': 0.9357836835017357}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:44:31,997] Trial 7 finished with value: 1.7752851294674041 and parameters: {'iterations': 592, 'learning_rate': 0.025429651471234556, 'depth': 4, 'l2_leaf_reg': 4.567524110826807, 'random_strength': 1.0303171909648792, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Lossguide'}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:47:16,074] Trial 8 finished with value: 1.9786862889820094 and parameters: {'iterations': 955, 'learning_rate': 0.03001628107185765, 'depth': 10, 'l2_leaf_reg': 8.252980387107725, 'random_strength': 1.944835568991828, 'bootstrap_type': 'Bayesian', 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:51:50,075] Trial 9 finished with value: 1.801117572521259 and parameters: {'iterations': 1934, 'learning_rate': 0.0071324481677087995, 'depth': 4, 'l2_leaf_reg': 2.027935709815214, 'random_strength': 1.6593914672581116, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Lossguide'}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:53:47,976] Trial 10 finished with value: 1.8159911374650222 and parameters: {'iterations': 534, 'learning_rate': 0.07928586959588013, 'depth': 8, 'l2_leaf_reg': 6.556672942598847, 'random_strength': 0.6304975226078839, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7313248927692078}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:55:24,388] Trial 11 finished with value: 1.7735599364855696 and parameters: {'iterations': 520, 'learning_rate': 0.037091599886253955, 'depth': 5, 'l2_leaf_reg': 5.7651373285109875, 'random_strength': 0.8389691296852178, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.98409086024973}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 16:58:10,744] Trial 12 finished with value: 1.7496048756427556 and parameters: {'iterations': 754, 'learning_rate': 0.03882291492958056, 'depth': 6, 'l2_leaf_reg': 6.407489782221685, 'random_strength': 0.6049137854712621, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9999152999282487}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:01:35,941] Trial 13 finished with value: 1.7903553480279735 and parameters: {'iterations': 824, 'learning_rate': 0.038900530051532906, 'depth': 7, 'l2_leaf_reg': 7.107232865495068, 'random_strength': 0.6049925256970744, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9987465555382867}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:03:54,850] Trial 14 finished with value: 1.7733718343448999 and parameters: {'iterations': 727, 'learning_rate': 0.07071400847819542, 'depth': 6, 'l2_leaf_reg': 5.714804839409675, 'random_strength': 1.710257228874465, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8198654061038251}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:09:05,386] Trial 15 finished with value: 1.7503066274096504 and parameters: {'iterations': 1348, 'learning_rate': 0.043155334109772646, 'depth': 8, 'l2_leaf_reg': 2.355989430900323, 'random_strength': 1.5046581237242698, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8730600977981406}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:11:04,706] Trial 16 finished with value: 1.775590685676864 and parameters: {'iterations': 828, 'learning_rate': 0.0944757062661858, 'depth': 5, 'l2_leaf_reg': 8.55719925498157, 'random_strength': 0.7700233000298469, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7069992022906731}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:15:06,316] Trial 17 finished with value: 1.7644811950741002 and parameters: {'iterations': 987, 'learning_rate': 0.04816248201198955, 'depth': 9, 'l2_leaf_reg': 6.90882724111443, 'random_strength': 1.7831350717504144, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7928879938882509}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:17:43,607] Trial 18 finished with value: 1.7568592503257323 and parameters: {'iterations': 744, 'learning_rate': 0.06925236662444952, 'depth': 6, 'l2_leaf_reg': 5.4405888793120365, 'random_strength': 1.2999177399862727, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9389743122366312}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:19:23,674] Trial 19 finished with value: 1.8241868976036806 and parameters: {'iterations': 1370, 'learning_rate': 0.021315984258015883, 'depth': 7, 'l2_leaf_reg': 9.934465930633468, 'random_strength': 0.7793157040737136, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'subsample': 0.6646013536069808}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:20:43,114] Trial 20 finished with value: 1.8190752610048548 and parameters: {'iterations': 1144, 'learning_rate': 0.08359730056722797, 'depth': 3, 'l2_leaf_reg': 3.1778964547544564, 'random_strength': 1.5502366783831083, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.5028398311319043}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:26:01,464] Trial 21 finished with value: 1.7512268227426888 and parameters: {'iterations': 1413, 'learning_rate': 0.04726677915416956, 'depth': 8, 'l2_leaf_reg': 1.7974286179825756, 'random_strength': 1.5266014388920273, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8734562631072419}. Best is trial 2 with value: 1.742345530997049.\n",
            "[I 2024-09-11 17:31:17,105] Trial 22 finished with value: 1.7408822480575572 and parameters: {'iterations': 1249, 'learning_rate': 0.03991094705446412, 'depth': 9, 'l2_leaf_reg': 2.7211801955454376, 'random_strength': 1.8357962340203944, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9986541935414813}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:35:32,548] Trial 23 finished with value: 1.7884363832476058 and parameters: {'iterations': 918, 'learning_rate': 0.03180887069966648, 'depth': 9, 'l2_leaf_reg': 3.5606446530074383, 'random_strength': 1.8508057028330849, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9615919806021926}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:39:49,138] Trial 24 finished with value: 1.7880842662518126 and parameters: {'iterations': 1180, 'learning_rate': 0.015981222419106267, 'depth': 6, 'l2_leaf_reg': 4.838257353042904, 'random_strength': 1.6708305311134986, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9926910528456506}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:42:54,045] Trial 25 finished with value: 1.7787113550908982 and parameters: {'iterations': 698, 'learning_rate': 0.05189267992991552, 'depth': 9, 'l2_leaf_reg': 2.643278580915921, 'random_strength': 1.8357312845552352, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9181833470230067}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:47:27,613] Trial 26 finished with value: 1.960556046159664 and parameters: {'iterations': 1578, 'learning_rate': 0.002726576473861958, 'depth': 5, 'l2_leaf_reg': 1.063451902145799, 'random_strength': 1.9111457597738537, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.879261311439648}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:49:57,476] Trial 27 finished with value: 1.7431092280662295 and parameters: {'iterations': 632, 'learning_rate': 0.06535359828881387, 'depth': 7, 'l2_leaf_reg': 4.092619829015035, 'random_strength': 0.5377661340582846, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9622023620070799}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:50:53,099] Trial 28 finished with value: 1.804759784431844 and parameters: {'iterations': 617, 'learning_rate': 0.09136766558377328, 'depth': 7, 'l2_leaf_reg': 4.0381978111435135, 'random_strength': 1.7553155533303118, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Depthwise'}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:51:53,589] Trial 29 finished with value: 1.9123420795806003 and parameters: {'iterations': 1043, 'learning_rate': 0.06254708462600063, 'depth': 9, 'l2_leaf_reg': 2.984046307771858, 'random_strength': 1.381856773236709, 'bootstrap_type': 'MVS', 'grow_policy': 'SymmetricTree', 'subsample': 0.9489749690315759}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:52:19,195] Trial 30 finished with value: 1.8454715218779605 and parameters: {'iterations': 874, 'learning_rate': 0.07289507471207302, 'depth': 4, 'l2_leaf_reg': 1.2819850036855596, 'random_strength': 1.6200383581813964, 'bootstrap_type': 'MVS', 'grow_policy': 'SymmetricTree', 'subsample': 0.7905972793592237}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:54:27,465] Trial 31 finished with value: 1.7854881083703609 and parameters: {'iterations': 708, 'learning_rate': 0.06348925136372827, 'depth': 5, 'l2_leaf_reg': 5.084179545644697, 'random_strength': 0.5617897929681361, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.967778457385598}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 17:56:54,017] Trial 32 finished with value: 1.7736871650602875 and parameters: {'iterations': 662, 'learning_rate': 0.03903850498746171, 'depth': 6, 'l2_leaf_reg': 6.099299318603148, 'random_strength': 0.7227694513484372, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9977514869208189}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:00:04,134] Trial 33 finished with value: 1.7841170031185707 and parameters: {'iterations': 811, 'learning_rate': 0.03150413543176099, 'depth': 7, 'l2_leaf_reg': 7.835327540478598, 'random_strength': 0.527955361325028, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9114611210162703}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:03:50,618] Trial 34 finished with value: 1.7698626042258598 and parameters: {'iterations': 1092, 'learning_rate': 0.0891284219232263, 'depth': 6, 'l2_leaf_reg': 3.777689404994069, 'random_strength': 1.0533778500420348, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9520066783320267}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:06:03,513] Trial 35 finished with value: 1.8591413430535426 and parameters: {'iterations': 1244, 'learning_rate': 0.05657293527585823, 'depth': 8, 'l2_leaf_reg': 4.247736443249209, 'random_strength': 0.9072976634404728, 'bootstrap_type': 'MVS', 'grow_policy': 'Depthwise', 'subsample': 0.841923240650471}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:06:44,157] Trial 36 finished with value: 1.8486568325602697 and parameters: {'iterations': 1023, 'learning_rate': 0.045399051153205514, 'depth': 7, 'l2_leaf_reg': 4.985487960432009, 'random_strength': 0.6785790498290001, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'subsample': 0.9005376078900107}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:08:31,860] Trial 37 finished with value: 1.8242039995255777 and parameters: {'iterations': 590, 'learning_rate': 0.07728013840211206, 'depth': 5, 'l2_leaf_reg': 3.3269042584960062, 'random_strength': 1.1702832955667168, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Lossguide'}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:13:53,429] Trial 38 finished with value: 1.7854278928241332 and parameters: {'iterations': 1536, 'learning_rate': 0.060549266500243185, 'depth': 10, 'l2_leaf_reg': 0.2189237509934645, 'random_strength': 0.900470464681997, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.9676481507486936}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:16:16,233] Trial 39 finished with value: 1.7886720969798253 and parameters: {'iterations': 1845, 'learning_rate': 0.023225201607071772, 'depth': 6, 'l2_leaf_reg': 2.6650508021600485, 'random_strength': 1.443234425076935, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'subsample': 0.9277578869807751}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:16:47,906] Trial 40 finished with value: 1.8609635463448149 and parameters: {'iterations': 768, 'learning_rate': 0.05308897118389223, 'depth': 7, 'l2_leaf_reg': 7.595216065081626, 'random_strength': 1.0564303647265914, 'bootstrap_type': 'Bayesian', 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:21:44,390] Trial 41 finished with value: 1.7706839429966426 and parameters: {'iterations': 1318, 'learning_rate': 0.04330058563660873, 'depth': 8, 'l2_leaf_reg': 2.150850713357422, 'random_strength': 1.4298370631932789, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8456403514547227}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:27:27,394] Trial 42 finished with value: 1.7762994470062927 and parameters: {'iterations': 1462, 'learning_rate': 0.03556421906075874, 'depth': 8, 'l2_leaf_reg': 2.72475206519956, 'random_strength': 1.540237735088494, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8795295778058555}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:34:14,875] Trial 43 finished with value: 1.7667738726291937 and parameters: {'iterations': 1747, 'learning_rate': 0.04188785523128434, 'depth': 9, 'l2_leaf_reg': 4.409856799379629, 'random_strength': 1.5856022110063834, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.97415138066504}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:39:34,722] Trial 44 finished with value: 1.7649898971163225 and parameters: {'iterations': 1231, 'learning_rate': 0.02755572640253516, 'depth': 8, 'l2_leaf_reg': 2.2624245407440857, 'random_strength': 1.982070573444704, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9993132448772949}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:43:19,138] Trial 45 finished with value: 1.7561458199700053 and parameters: {'iterations': 910, 'learning_rate': 0.04997305356694119, 'depth': 8, 'l2_leaf_reg': 1.2365702219816483, 'random_strength': 1.2839919014377892, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9500078725752743}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:45:36,793] Trial 46 finished with value: 1.8076538401672853 and parameters: {'iterations': 508, 'learning_rate': 0.09903094398662388, 'depth': 10, 'l2_leaf_reg': 6.202588097805011, 'random_strength': 0.5106173413214943, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9006902567869123}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:50:40,925] Trial 47 finished with value: 1.7766449125915313 and parameters: {'iterations': 1341, 'learning_rate': 0.0414081921035687, 'depth': 7, 'l2_leaf_reg': 1.6291615570890083, 'random_strength': 1.714922045307692, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9295881362490416}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:56:14,937] Trial 48 finished with value: 1.7534408002029547 and parameters: {'iterations': 1497, 'learning_rate': 0.06708009962557346, 'depth': 9, 'l2_leaf_reg': 3.7805319414393823, 'random_strength': 1.4738875329413277, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9746603166763961}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 18:58:38,218] Trial 49 finished with value: 1.78343777762751 and parameters: {'iterations': 649, 'learning_rate': 0.0353173736638754, 'depth': 6, 'l2_leaf_reg': 0.46062444685921333, 'random_strength': 1.3571661537520316, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Lossguide'}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:00:22,437] Trial 50 finished with value: 1.7710005454062876 and parameters: {'iterations': 1113, 'learning_rate': 0.08347358975398099, 'depth': 7, 'l2_leaf_reg': 5.493070126914828, 'random_strength': 0.6444944739613576, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Depthwise', 'subsample': 0.9729017132590838}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:05:39,853] Trial 51 finished with value: 1.7669956915545733 and parameters: {'iterations': 1432, 'learning_rate': 0.04641958884967974, 'depth': 8, 'l2_leaf_reg': 1.8672876927107291, 'random_strength': 1.5200543068522192, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8067991796917804}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:11:43,669] Trial 52 finished with value: 1.7719612181288709 and parameters: {'iterations': 1666, 'learning_rate': 0.057535888314834985, 'depth': 8, 'l2_leaf_reg': 2.397090901704785, 'random_strength': 1.189260442643016, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8707785034089908}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:17:16,344] Trial 53 finished with value: 1.7748572553263173 and parameters: {'iterations': 1400, 'learning_rate': 0.04604590883990611, 'depth': 9, 'l2_leaf_reg': 3.355957129690855, 'random_strength': 1.6284515404043343, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8561051489107465}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:21:32,622] Trial 54 finished with value: 1.7749899439148251 and parameters: {'iterations': 1265, 'learning_rate': 0.03350603268079961, 'depth': 7, 'l2_leaf_reg': 1.7333618747692627, 'random_strength': 1.7489802471498521, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7599355081734772}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:25:56,743] Trial 55 finished with value: 1.7835819689515346 and parameters: {'iterations': 1653, 'learning_rate': 0.05053783494709685, 'depth': 6, 'l2_leaf_reg': 0.8565230146594303, 'random_strength': 1.870909293988292, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.6471630923044353}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:30:15,954] Trial 56 finished with value: 1.7869363444446171 and parameters: {'iterations': 975, 'learning_rate': 0.03886783250586971, 'depth': 9, 'l2_leaf_reg': 2.962169745413961, 'random_strength': 1.8093726156943473, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9409889320827995}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:32:31,705] Trial 57 finished with value: 1.805293587684173 and parameters: {'iterations': 578, 'learning_rate': 0.018950561896863505, 'depth': 8, 'l2_leaf_reg': 4.602192479467256, 'random_strength': 1.6641200987331082, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.7630068230107299}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:33:18,528] Trial 58 finished with value: 1.828111553539299 and parameters: {'iterations': 1191, 'learning_rate': 0.05361255785374487, 'depth': 7, 'l2_leaf_reg': 6.920322181234911, 'random_strength': 1.4801774639883636, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'subsample': 0.8988729936852307}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:35:54,937] Trial 59 finished with value: 1.7964190828722 and parameters: {'iterations': 851, 'learning_rate': 0.02921694312509103, 'depth': 5, 'l2_leaf_reg': 4.133842786515801, 'random_strength': 1.3661059697679543, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9831841167511656}. Best is trial 22 with value: 1.7408822480575572.\n",
            "[I 2024-09-11 19:40:36,442] Trial 60 finished with value: 1.738955275182579 and parameters: {'iterations': 1346, 'learning_rate': 0.042390909851230944, 'depth': 8, 'l2_leaf_reg': 1.5023079004958326, 'random_strength': 1.918922766073629, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7191772414063006}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 19:45:16,064] Trial 61 finished with value: 1.7645112445052002 and parameters: {'iterations': 1325, 'learning_rate': 0.04349306831514538, 'depth': 8, 'l2_leaf_reg': 1.5487155397227421, 'random_strength': 1.908928397324621, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7232684693305009}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 19:50:00,225] Trial 62 finished with value: 1.771795467692336 and parameters: {'iterations': 1407, 'learning_rate': 0.049062607918651024, 'depth': 8, 'l2_leaf_reg': 2.4375463031154485, 'random_strength': 0.5875125521451462, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.683713240638548}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 19:53:02,738] Trial 63 finished with value: 1.762400174354343 and parameters: {'iterations': 772, 'learning_rate': 0.03828495001715685, 'depth': 9, 'l2_leaf_reg': 0.5870471849737524, 'random_strength': 1.948069456428118, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7364798675410957}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 19:57:41,899] Trial 64 finished with value: 1.7484429918043227 and parameters: {'iterations': 1492, 'learning_rate': 0.07481139172259921, 'depth': 6, 'l2_leaf_reg': 3.4656057022930415, 'random_strength': 1.7707502525064431, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8221605923415505}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:01:02,681] Trial 65 finished with value: 1.7735195417350842 and parameters: {'iterations': 1292, 'learning_rate': 0.07607032152982762, 'depth': 6, 'l2_leaf_reg': 3.456751478997054, 'random_strength': 1.7826345523321165, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.6253720478875271}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:04:57,705] Trial 66 finished with value: 1.7950005921268208 and parameters: {'iterations': 1524, 'learning_rate': 0.08705950894448855, 'depth': 5, 'l2_leaf_reg': 2.968389788135095, 'random_strength': 1.7129590118802354, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.781555412931191}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:06:59,730] Trial 67 finished with value: 1.7893880684921624 and parameters: {'iterations': 1615, 'learning_rate': 0.0938736684599267, 'depth': 6, 'l2_leaf_reg': 3.9776140481695768, 'random_strength': 1.8800566027494194, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Depthwise'}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:12:03,384] Trial 68 finished with value: 1.742636650104745 and parameters: {'iterations': 1472, 'learning_rate': 0.06685528740840488, 'depth': 7, 'l2_leaf_reg': 4.792263372365624, 'random_strength': 1.7990594196815857, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8275827848863286}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:16:07,578] Trial 69 finished with value: 1.7409731613636725 and parameters: {'iterations': 1481, 'learning_rate': 0.08062654633864451, 'depth': 7, 'l2_leaf_reg': 4.668116700251796, 'random_strength': 1.9739739399866494, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.7027951889249898}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:20:46,094] Trial 70 finished with value: 1.819633651721865 and parameters: {'iterations': 1754, 'learning_rate': 0.07994842181285874, 'depth': 7, 'l2_leaf_reg': 4.542228006839916, 'random_strength': 1.9818030252356997, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.6970846703292748}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:24:19,855] Trial 71 finished with value: 1.8167282159307252 and parameters: {'iterations': 1477, 'learning_rate': 0.07365156077722029, 'depth': 6, 'l2_leaf_reg': 5.341840022087301, 'random_strength': 1.8264416006214597, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.5798984336446684}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:28:29,469] Trial 72 finished with value: 1.764601757438101 and parameters: {'iterations': 1568, 'learning_rate': 0.08121738983357568, 'depth': 7, 'l2_leaf_reg': 4.855901722079968, 'random_strength': 1.9126928232511868, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.6770904723750388}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:32:22,869] Trial 73 finished with value: 1.7652655141664395 and parameters: {'iterations': 1368, 'learning_rate': 0.06910327936383814, 'depth': 6, 'l2_leaf_reg': 3.590009618075725, 'random_strength': 1.7891295287670215, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.739097743409825}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:36:27,425] Trial 74 finished with value: 1.7458388154355249 and parameters: {'iterations': 1456, 'learning_rate': 0.0661535739344708, 'depth': 6, 'l2_leaf_reg': 5.864184017336381, 'random_strength': 1.9437298652117414, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.7139362723998145}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:40:38,685] Trial 75 finished with value: 1.7949290525165085 and parameters: {'iterations': 1459, 'learning_rate': 0.0661310951463903, 'depth': 7, 'l2_leaf_reg': 5.867475622468623, 'random_strength': 1.943481695179735, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.7102189433943709}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:41:26,213] Trial 76 finished with value: 1.8265593957249588 and parameters: {'iterations': 1540, 'learning_rate': 0.060084103979345105, 'depth': 6, 'l2_leaf_reg': 5.236322406300017, 'random_strength': 1.8593835901737121, 'bootstrap_type': 'MVS', 'grow_policy': 'SymmetricTree', 'subsample': 0.6532393437424994}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:45:46,206] Trial 77 finished with value: 1.7885559786210568 and parameters: {'iterations': 1629, 'learning_rate': 0.07348355683099225, 'depth': 5, 'l2_leaf_reg': 4.714796795749294, 'random_strength': 1.7479139874843115, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.8146733724947594}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:49:55,409] Trial 78 finished with value: 1.7761439661912415 and parameters: {'iterations': 1499, 'learning_rate': 0.08662827285814115, 'depth': 7, 'l2_leaf_reg': 4.296302120145795, 'random_strength': 1.9943935717908465, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.7230578759277624}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:54:15,318] Trial 79 finished with value: 1.758994578199088 and parameters: {'iterations': 1438, 'learning_rate': 0.06589455858700577, 'depth': 7, 'l2_leaf_reg': 6.522656559610642, 'random_strength': 1.9312873690324315, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.7490140862959818}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:55:50,395] Trial 80 finished with value: 1.784196507349536 and parameters: {'iterations': 1384, 'learning_rate': 0.07684739919212767, 'depth': 6, 'l2_leaf_reg': 5.668565981936283, 'random_strength': 1.8895061116204843, 'bootstrap_type': 'MVS', 'grow_policy': 'Depthwise', 'subsample': 0.7021019648959346}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 20:58:23,934] Trial 81 finished with value: 1.7849973146209857 and parameters: {'iterations': 702, 'learning_rate': 0.07185083323886407, 'depth': 6, 'l2_leaf_reg': 6.265413663659349, 'random_strength': 1.859155369083621, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9894124600177352}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:00:06,179] Trial 82 finished with value: 1.785686947066519 and parameters: {'iterations': 549, 'learning_rate': 0.09478252269183643, 'depth': 6, 'l2_leaf_reg': 3.8717877984312494, 'random_strength': 1.8174523468182777, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7717052689757814}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:02:15,016] Trial 83 finished with value: 1.7458345490491807 and parameters: {'iterations': 802, 'learning_rate': 0.0839142027137279, 'depth': 5, 'l2_leaf_reg': 7.332946464361853, 'random_strength': 1.960602752724913, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8257950329993998}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:05:27,155] Trial 84 finished with value: 1.7515065004356396 and parameters: {'iterations': 1554, 'learning_rate': 0.09105213830557064, 'depth': 4, 'l2_leaf_reg': 8.852745253222297, 'random_strength': 1.9502780909646005, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8013547516581161}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:07:41,303] Trial 85 finished with value: 1.8291076968740814 and parameters: {'iterations': 934, 'learning_rate': 0.08405364023081557, 'depth': 4, 'l2_leaf_reg': 7.468942621131946, 'random_strength': 1.9969976204422513, 'bootstrap_type': 'Bayesian', 'grow_policy': 'Lossguide'}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:11:10,837] Trial 86 finished with value: 1.814187141478027 and parameters: {'iterations': 1300, 'learning_rate': 0.07997577689222111, 'depth': 5, 'l2_leaf_reg': 5.106066584602364, 'random_strength': 1.6826315218704422, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8286920201207046}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:12:35,847] Trial 87 finished with value: 1.858827256060777 and parameters: {'iterations': 1207, 'learning_rate': 0.0630752021259785, 'depth': 3, 'l2_leaf_reg': 3.1446967836260695, 'random_strength': 1.7695028369275867, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.5169882588247061}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:18:48,719] Trial 88 finished with value: 1.7985030344587918 and parameters: {'iterations': 1594, 'learning_rate': 0.06948200009007344, 'depth': 10, 'l2_leaf_reg': 3.6506901456352177, 'random_strength': 1.9551676364041384, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8291912067506216}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:19:21,014] Trial 89 finished with value: 1.8112148699233712 and parameters: {'iterations': 1136, 'learning_rate': 0.0755988001650379, 'depth': 5, 'l2_leaf_reg': 9.379991833847452, 'random_strength': 1.8352702778826637, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'subsample': 0.690168229573894}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:21:46,168] Trial 90 finished with value: 1.772584478300057 and parameters: {'iterations': 651, 'learning_rate': 0.08706143123436037, 'depth': 7, 'l2_leaf_reg': 8.097562061054749, 'random_strength': 1.8847272185052701, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.8550519384176436}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:24:35,113] Trial 91 finished with value: 1.7912146558352051 and parameters: {'iterations': 792, 'learning_rate': 0.08174720934268186, 'depth': 6, 'l2_leaf_reg': 7.258331049447993, 'random_strength': 0.7188057314191955, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9580309434269414}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:27:16,253] Trial 92 finished with value: 1.796701212084265 and parameters: {'iterations': 739, 'learning_rate': 0.0784453176353636, 'depth': 6, 'l2_leaf_reg': 6.753823093755901, 'random_strength': 1.7338770265851056, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9811301528119586}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:30:05,899] Trial 93 finished with value: 1.7797243392752988 and parameters: {'iterations': 865, 'learning_rate': 0.04069150146881764, 'depth': 7, 'l2_leaf_reg': 4.477741925329677, 'random_strength': 0.5442071122206253, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7183592974642952}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:32:35,660] Trial 94 finished with value: 1.7899759523049243 and parameters: {'iterations': 680, 'learning_rate': 0.05931294937908444, 'depth': 6, 'l2_leaf_reg': 5.82264199188497, 'random_strength': 1.9114521869111631, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.9952295101128825}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:34:10,327] Trial 95 finished with value: 1.774762717378574 and parameters: {'iterations': 623, 'learning_rate': 0.08961793427527348, 'depth': 5, 'l2_leaf_reg': 6.000217973762763, 'random_strength': 1.8013694796026871, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7496666165550852}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:38:35,043] Trial 96 finished with value: 1.8050327841150333 and parameters: {'iterations': 1440, 'learning_rate': 0.09661370773764427, 'depth': 7, 'l2_leaf_reg': 6.474242133566406, 'random_strength': 1.6215579861253502, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.6718371123075071}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:43:35,680] Trial 97 finished with value: 1.7429841634146 and parameters: {'iterations': 1504, 'learning_rate': 0.056202378152426465, 'depth': 6, 'l2_leaf_reg': 4.113367185704384, 'random_strength': 1.002045666198979, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.9178561058183814}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:46:59,321] Trial 98 finished with value: 1.7802668055386677 and parameters: {'iterations': 1501, 'learning_rate': 0.05591116060735102, 'depth': 4, 'l2_leaf_reg': 4.106976787523564, 'random_strength': 0.8140418620299562, 'bootstrap_type': 'MVS', 'grow_policy': 'Lossguide', 'subsample': 0.8904439989633939}. Best is trial 60 with value: 1.738955275182579.\n",
            "[I 2024-09-11 21:48:36,638] Trial 99 finished with value: 1.8136457181146082 and parameters: {'iterations': 1042, 'learning_rate': 0.06418791037805537, 'depth': 7, 'l2_leaf_reg': 3.1899410339591223, 'random_strength': 0.9812885314618945, 'bootstrap_type': 'MVS', 'grow_policy': 'Depthwise', 'subsample': 0.9346928423195063}. Best is trial 60 with value: 1.738955275182579.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'iterations': 1346, 'learning_rate': 0.042390909851230944, 'depth': 8, 'l2_leaf_reg': 1.5023079004958326, 'random_strength': 1.918922766073629, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'Lossguide', 'subsample': 0.7191772414063006}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to optimize CatBoost parameters using Optuna\n",
        "def objective(trial):\n",
        "    # Define hyperparameter search space\n",
        "    catboost_params = {\n",
        "        'iterations': trial.suggest_int('iterations', 500, 2000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
        "        'depth': trial.suggest_int('depth', 3, 10),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10),\n",
        "        'random_strength': trial.suggest_float('random_strength', 0.5, 2.0),\n",
        "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
        "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
        "        'silent': True\n",
        "    }\n",
        "\n",
        "    # Only use subsample if bootstrap type is not 'Bayesian'\n",
        "    if catboost_params['bootstrap_type'] != 'Bayesian':\n",
        "        catboost_params['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
        "\n",
        "    # Initialize the CatBoostRegressor with trial parameters\n",
        "    model = CatBoostRegressor(**catboost_params)\n",
        "\n",
        "    # Train and validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    y_train_log = np.log1p(y_train)\n",
        "    y_val_log = np.log1p(y_val)\n",
        "\n",
        "    # Scale the features\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train_scaled, y_train_log)\n",
        "\n",
        "    # Predict on validation set\n",
        "    val_preds_log = model.predict(X_val_scaled)\n",
        "    val_preds = np.expm1(val_preds_log)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    val_rmse = np.sqrt(mean_squared_error(np.expm1(y_val_log), val_preds))\n",
        "\n",
        "    return val_rmse\n",
        "\n",
        "# Run Optuna optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = study.best_params\n",
        "print(f'Best parameters: {best_params}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "H24igVDd5mBS",
        "outputId": "cd1275b9-55dc-48cb-f662-5c61028a0f39"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-55dea9a629ed>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# Align test set columns with training set (handle any potential missing columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4542\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4543\u001b[0m         \"\"\"\n\u001b[0;32m-> 4544\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4545\u001b[0m         return self._constructor(new_values, index=self.index, copy=False).__finalize__(\n\u001b[1;32m   4546\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-55dea9a629ed>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# Align test set columns with training set (handle any potential missing columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"OUS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mapping)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mis_scalar_nan\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# Function to reduce memory usage\n",
        "def reduce_memory_usage(df):\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not isinstance(col_type, pd.CategoricalDtype):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "    return df\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.merge(users_train, user_features_train, on='ID')\n",
        "train_data = pd.merge(train_data, targets_train, on='ID')\n",
        "\n",
        "# Label encode categorical variables\n",
        "label_encoders = {}\n",
        "for column in train_data.select_dtypes(include=['category', 'object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column].astype(str))\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Feature engineering - Date-based features\n",
        "train_data['first_open_datetime'] = pd.to_datetime(train_data['first_open_timestamp'], unit='us')\n",
        "train_data['first_open_day'] = train_data['first_open_datetime'].dt.day\n",
        "train_data['first_open_month'] = train_data['first_open_datetime'].dt.month\n",
        "train_data['first_open_hour'] = train_data['first_open_datetime'].dt.hour\n",
        "\n",
        "# Aggregated behavioral features\n",
        "train_data['total_retention'] = train_data[[f'RetentionD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "train_data['total_levels_completed'] = train_data[[f'LevelAdvancedCountD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "train_data['total_ad_revenue'] = train_data[[f'AdRevenueD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "train_data['total_iap_revenue'] = train_data[[f'IAPRevenueD{i}' for i in range(1, 16)]].sum(axis=1)\n",
        "\n",
        "# Drop unnecessary columns (including datetime columns)\n",
        "useless_columns = ['ID', 'first_open_date', 'first_open_timestamp', 'local_first_open_timestamp', 'first_open_datetime']\n",
        "train_data.drop(columns=useless_columns, inplace=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = train_data.drop(columns=['TARGET'])\n",
        "y = train_data['TARGET']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical features\n",
        "cat_features = ['country', 'platform', 'device_category', 'device_brand', 'device_model', 'ad_network']\n",
        "\n",
        "# Separate numerical features for scaling\n",
        "num_features = [col for col in X_train.columns if col not in cat_features]\n",
        "\n",
        "# Scale the numerical features only\n",
        "scaler = StandardScaler()\n",
        "X_train_num_scaled = scaler.fit_transform(X_train[num_features])\n",
        "X_val_num_scaled = scaler.transform(X_val[num_features])\n",
        "\n",
        "# Combine scaled numerical features with original categorical features (categorical features are not scaled)\n",
        "X_train_combined = pd.DataFrame(X_train_num_scaled, columns=num_features)\n",
        "X_train_combined[cat_features] = X_train[cat_features].reset_index(drop=True)\n",
        "\n",
        "X_val_combined = pd.DataFrame(X_val_num_scaled, columns=num_features)\n",
        "X_val_combined[cat_features] = X_val[cat_features].reset_index(drop=True)\n",
        "\n",
        "# Apply log transformation to the target\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_val_log = np.log1p(y_val)\n",
        "\n",
        "# Convert the data into CatBoost Pool format, making sure categorical features are passed correctly\n",
        "train_pool = Pool(X_train_combined, y_train_log, cat_features=cat_features)\n",
        "val_pool = Pool(X_val_combined, y_val_log, cat_features=cat_features)\n",
        "\n",
        "# Best parameters from Optuna tuning\n",
        "best_params = {\n",
        "    'iterations': 1346,\n",
        "    'learning_rate': 0.042390909851230944,\n",
        "    'depth': 8,\n",
        "    'l2_leaf_reg': 1.5023079004958326,\n",
        "    'random_strength': 1.918922766073629,\n",
        "    'bootstrap_type': 'Bernoulli',\n",
        "    'grow_policy': 'Lossguide',\n",
        "    'subsample': 0.7191772414063006,\n",
        "    'silent': True,\n",
        "    'early_stopping_rounds': 100,  # Early stopping\n",
        "    'loss_function': 'RMSE'\n",
        "}\n",
        "\n",
        "# Train final CatBoost model with the best parameters\n",
        "best_catboost_model = CatBoostRegressor(**best_params)\n",
        "best_catboost_model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "# Test set processing\n",
        "test_data = pd.merge(users_test, user_features_test, on='ID')\n",
        "\n",
        "# Apply label encoding to the test set using the same encoders as in the training set\n",
        "for column in test_data.select_dtypes(include=['category', 'object']).columns:\n",
        "    le = label_encoders.get(column)\n",
        "    if le is not None:\n",
        "        test_data[column] = test_data[column].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
        "\n",
        "# Align test set columns with training set (handle any potential missing columns)\n",
        "test_data = test_data.reindex(columns=X_train_combined.columns, fill_value=0)\n",
        "\n",
        "# Separate numerical features for scaling\n",
        "test_data_num_scaled = scaler.transform(test_data[num_features])\n",
        "\n",
        "# Combine scaled numerical features with original (unscaled) categorical features for test data\n",
        "test_data_combined = pd.DataFrame(test_data_num_scaled, columns=num_features)\n",
        "test_data_combined[cat_features] = test_data[cat_features].reset_index(drop=True)\n",
        "\n",
        "# Make predictions on the test set using the final CatBoost model\n",
        "test_pool = Pool(test_data_combined, cat_features=cat_features)\n",
        "test_preds_log = best_catboost_model.predict(test_pool)\n",
        "test_preds = np.expm1(test_preds_log)\n",
        "\n",
        "# Create a submission file\n",
        "submission = pd.DataFrame({'ID': users_test['ID'], 'TARGET': test_preds})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Assuming 'TARGET' is available in the test data for RMSE calculation\n",
        "if 'TARGET' in test_data.columns:\n",
        "    test_rmse = np.sqrt(mean_squared_error(test_data['TARGET'], test_preds))\n",
        "    print(f'Final RMSE on the test set: {test_rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-RCzlo9mxn3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}